# Backlog

|Priority (L/M/H)|Topic          |Category       |Effort (L/M/H)|Collected notes|
------------- |-------------| ---- | --- | --- 
H|Merge 'Languages' and 'Frameworks' sections| - |M| - 
L|WCF connection patterns|Languages/C#|M|`Dispose` vs `Close`+`Abort`, proper use `ChannelFactory<T>`, caching (what and why)
L|SCIM2|Architecture/General|L|SCIM2 is an API open standard for cross domain identity management
L|PIM/PAM|Architecture/General|M|Class of solutions that manage elevated permission accounts and their sessions (ie. admins, super-users and so on)
M|CMM|Architecture/General|M|*Capability Maturity Model* - development model for assessing how mature particular processes are in certain organization/department/project; this can span from things like DevOps maturity, cloud-readiness, quality gates, code-review, self-organisation etc; measurement ("scoring") spans from 1 (Initial) to 5 (Effective)
H|SQL Server locking debugging|Languages/SQL|L|Expand current *T-SQL Performance Analysis Cheatsheet* with `sp_lock` and `sp_who` + SPID
M|XAdES|Engineering/Security|H|XML signing - expand on how it actually works "underneath"; used for **qualified electronic signature** (recommended by UE and MSWiA in Poland)
H|SQL Server - `UPDLOCK` vs `SERIALIZABLE`|Languages/SQL|M|`UPDLOCK` hint can be used instead of `SERIALIZABLE` isolation level to prevent duplicates `INSERT` in (`SELECT` then `INSERT` scenario); it can gracefully force other sessions (that also use this hint) to wait for their "chance" to SELECT without ending up with deadlocks (as in `SERIALIZABLE` case)
M|SQL Server - lock compatibility matrix|Languages/SQL|M|Describe compatibility matrix between Shared (S), Exclusive (X), Update (U) and all the Intent types (I...)
M|SQL Server - `RoundTimeDown`|Languages/SQL|L|Describe creating time buckets in SQL Server with `RoundTimeDown`; quite handy for reporting and all time series oriented data in SQL Server
M|OpenTelemetry|Architecture/General|H|Describe OpenTelemetry - it's goals, current state and usage; from their page it `is a collection of tools, APIs, and SDKs. You can use it to instrument, generate, collect, and export telemetry data (metrics, logs, and traces) for analysis in order to understand your software's performance and behavior.`
M|Microfrontends|Architecture/Distributed|H|Describe Microfrontends as a general approach to decomposing monolithic frontends and as a final step in *vertical slices* approach to succeed
L|Webpack Module Federation vs single-spa|Languages/JavaScript|H|Describe differences between these two approaches and how they can be used to achieve microfrontends when needed
M|TailwindCSS|Languages/CSS|H|TailwindCSS has been gathering large popularity for a long time already; investigate and write down in your words how it works and "why the hype"
L|StencilJS|Languages/JS|H|StencilJS has been recommended in many places as a go-to solution for Web Components (and building design systems with them); investigate and write down how in general it works
L|RBAC,ABAC,ACL|Architecture/General|M|Briefly desribe RBAC, ABAC (RBAC + attributes) and RBAC (business role - create, write, enslist, pay etc.) vs ACL (technical - has or has not access to resource)
M|Marshalling|Engineering/Distributed computing|M|Briefly desribe definition of marshalling and find few examples of it like RPC/REST (with JSON/XML/MessagePack etc.), COM etc; compare with serialization
H|Linux commands cheatsheet|Languages/Linux commands|M|Describe commonly used Linux commands, especially `find`, `grep`, `sed`, `awk`, `cat`, `touch`, `mkdir`, `ls`, `virtualenv` and others
M|x86 assembly basics|Languages/Assembly|H|"Serialize" notes from reading ***C.O.D.E.***; note down popular x86 opcodes to be not so ignorant when reading x86 ASM files
H|How computer works? - high level|Engineering/Computer Science|H|Create simple cheatsheet (with diagrams) on how computer architecture works, ie. CPU (ALU, registers and L1, L2, L3 cache), memory, disc, bus speed and ideas/problems around it, ie. cache misses, branch prediction, register spilling (moving to RAM) etc.
L|Shift left|Architecture/General|L|"Shift left" is an organisation approach to security, that encourages security review as early in development process as possible (so "left" means literally left on time axis given software development lifetime)
L|Sonar vs Fortify|Standalone software/CICD|H|Both tools are used for static code analysis; Sonar is used for for code quality analysis (and measuring technological debt), whereas Fortify is used for code security analysis; compare how they can be used together in CI/CD pipeline
H|Certificates in PKCS|Engineering/Security|H|Describe what role certificates play in Public Key Cryptography Standards (PKCS) infrastructure; describe what thumbrint (hashed certificate), signature, CA (Certificate Authority) and trust chain is; throwing in few words about asymmetric cryptography wouldn't hurt
M|Array pooling with `ArrayPool<T>`|Languages/C#|M|*Array pooling* is a concept of initializing large array beforehand (if we anticipate that we will be allocating large arrays and do it a lot) and then renting chunks from it when we need them; if we keep renting from once preallocated pool, we do not end up with lots large arrays on LOH (Large Object Heap), that can later on cause forced managed heap collection (which hurts performance a lot); `ArrayPool<T>` offers pooling and renting (with `ArrayPool<T>.Rent`) functionality in .NET and is thread-safe; Adam Sitnik has amazing post about it on https://adamsitnik.com/Array-Pool/
H|Small Object Heap (G0, G1, G2) and Large Object Heap|Languages/C#|H|Write down general mechanism behind .NET Garbage Collection, SOH (and its generations) and LOH; include topics like LOH compacting, memory fragmentation problem, when object lands on LOH and general LOH problems; definitely approach with Konrad Kokosa's book accompanying you
L|tcpdump on Linux|Engineering/Computer networks|M|Describe usage of tcpdump - network sniffer for Linux; could be worth comparing it briefly with Wireshark
L|TLS termination|Engineering/Computer networks|M|Describe TLS/SSL termination - how it works, why we do this (performance, network appliance, package analysis etc.)
L|FQDN vs hostname|Engineering/Computer networks|L|Hostname could be FQDN if it goes up to the top-level domain
M|Load balancing - L4 vs L7|Engineering/Computer networks|H|Describe how load balancing is implemented nowadays, especially comparing layer 4 and layer 7 balancing; if it ends up as not a technical but more general note, move to Architecture/Distributed
M|HAProxy|Standalone software|M|Play around with HAProxy and note some general concepts behind how it works
H|async-await vs locking|Languages/C#|M|Write down approaches to using async-await with locking (mainly `SemaphoreSlim`, but could also hackaround your way with simple bit flag and `Interlocked.CompareExchange`)
M|Raft algorithm|Engineering/Distributed computing|H|Describe Raft algorithm and how it achieves cluster consensus with distributed log, leader election etc; note actual uses in solutions like Neo4j (+ONgDB) or RabbitMQ (with quorum queues)
L|Byzantine generals problem (Byzantine fault)|Engineering/Computer science|M|Describe Byzantine fault problem and how it affects distributed computing systems
H|Event schema changes - challenges|Architecture/Distributed|M|Describe solutions to adapt to changing event schemas, especially with event sourcing approach, where multiple versions changing over time can't be guaranteed to properly deserialize to strongly-typed event type; solutions I can think of would be *upcasting* along with usage of *weak schema* (for ex. JSON); consider describing *snapshot* creation too
L|OSINT|Architecture/General|M|Describe (from high level perspective) what open-source intelligence approach is and why you should care in terms of information security
M|Business validation - close to input or in core domain|Architecture/General|M|Describe current thoughts on (especially basic, ie. form) validations outside of core business domain with libraries like `FluentValidation` or `MVC Model validation`, that can offload trivial checks like required, min/max length, pattern match etc; discuss challenges of (possibly) missed rule enforcement in domain model, that can lead to breaking invariants and persisting domain model in invalid state 
H|Angular - change tracking strategies|Languages/TypeScript/Angular|H|Describe how change tracking strategies work in Angular: *default* (with "deeper" checking and mutability support), *OnPush* (shallow, preferred for immutable data) and *disabled* (and manualy triggering using `ChangeDetectionRef`); describe briefly how `Zone.js` works with Angular; decribe how change detection mechanism work with templates (`foreach` + every *template binding* + compare previous value of field with current value but **only for those that are used in bindings**)
M|`[ThreadStatic]` vs `ThreadLocal<T>`|Languages/C#|M|Describe diffferences between `[ThreadStatic]` and `ThreadLocal<T>`; both are used for thread exclusive data (so each thread has its own copy), but `[ThreadStatic]` initializes for first thread only whereas `ThreadLocal<T>` initializes for every thread; `ThreadLocal<T>` also implements `IDisposable` (good for some cleanup); for keeping data local only to particular async flow (async-await chaing) you can also use `AsyncLocal<T>`
M|C# 9 records|Languages/C#|M|Describe how `record` - new C# (9.0) language feature - works; in general records have been introduced for implementing (reference) types, that are described by their data (so for ex. DTOs fit in here), offering out-of-the-box value equality. While meant mostly for immutable scenarios (by default they are immutable with auto-properties with `init` only setters), they can also be mutable. Records support nondestructive mutation (more like pure functional) with `with` keyword, that copy source record and apply given mutation on it, ie. `var secondRecord = firstRecord with { MyProp = "new value" }`. Another great productivity shorcut that records provide is a *positional syntax*, which allows us to define data-type in single line, ie: `public record Person(string firstName, string lastName, DateTime birthDate);` would yield encapsulated, immutable reference type with value equality implemented behind the scenes.
S|Angular - compare *dirty*, *touched* and *updateValueAndValidity*|Languages/TypeScript/Angular|M|Compare *dirty*, *touched* and *updateValueAndValidity* in Angular Forms and describe when and why should each of them be used.
H|Describe formal difference between *system* and *business* analysis|Architecture/Business analysis|M|Describe differences between system and business analysis. It seems that definition of business/system *analytic* and business/system *analysis* differ, as system analysis (according to Wikipedia and some other pages) is a subset of business analysis, which deals with translating organisation's needs, models and workflows into IT systems functionalities. On the other hand, it seems that business analytic does not deal directly with system analysis, even though it's a part of business analysis as a whole. Perhaps for practical reasons system analytic's work has been extracted from business analytic's responsibility list to allow division of work (and also specialization).
M|*Service blueprint* technique|Architecture/Business analysis|M|Describe *service blueprint* technique on how to plot a diagram of how particular service (let's take order as an example: order->pay->package->delivery) is being realized from high level perspective. Service blueprint consits of actors (placed on vertical/Y axis) and steps to deliver service (placed on horizontal/X axis). It also includes visibility boundry, which allows to distinguish processes that are visible to user (above boundry) and those, that happen "behind the scenes" (below boundry).
H|*Replication events* streaming via audit log or change tracking/change data capture|Architecture/Distributed|H|Describe my approach to building local storages around autonomous systems/modules/microservices etc. with audit log scanning or - if available - scanning Change Tracking (SQL Server) or Change Data Capture (SQL Server or PostgreSQL) and publishing results as either strongly typed (if in homogenous environment) or a weakly typed (if in polyglot environment) replication events.
L|`AsyncLock` by Stephen Cleary based on Stephen Toub MSDN article|Languages/C#|M|Describe a `AsyncLock` implementation, that simplifies mutual exclusion in async-await scenario (using `SemaphoreSlim` and `IDisposable` with `using` pattern behind the scenes)
H|Mutex, lock, semaphore, monitor, barrier, spinlock, RCU locks|Engineering/Computer science|H|Describe synchronization implementations used nowadays, including mutex, lock, semaphore, monitor, memory barrier/fence, spinlock, read-copy-update and readers-writer locks. It wouldn't hurt to mention read-modify-write, fetch-and-add and test-and-set operations. Why `volatile` keyword is important should also be mentioned here.
H|Compare HTTP Codes vs 200 + `{ errors: [] }` approach to business error handling|Architecture/Distributed|M|Discuss on how and when HTTP code for each class of problem (4xx) in RESTful APIs would be a better choice than 200s + errors in payload. Perhaps there should be some middle ground (especially for business validations) introduced as *catch-all* mechanism? I can think of returning every business error as 422 (Unprocessable Entity) to indicate business error (monitoring or client-side libraries auto-detect 4xx and 5xx as errors) along with payload decribing what went wrong. I'm typically against using HTTP codes in general as results of business rules violation, because I treat them as *technical errors*. So either input was malformed (serialization issue?), server refused to authorise user or simply such endpoint does not exist. Or maybe server just exploded and all we know is that 500 - Internal Server Error. However, I can understand how this can be misleading, that API returns 200 - OK for errors, even if there is a reason in a payload. When exposed to the outside world, I would probably go with 422 route + `{ errors: []}` approach.
M|SQL Server security model|Engineering/Databases|L|Describe basics of SQL Server security model: login, mapping login to user on particular database, creating role on such database, assigning user to it and assigning permissions to certain objects to that role.
M|What is Istio?|Engineering/Distributed|M|Describe what is Istio and what problems it solves, ie. traffic management (routing, policies, load balancing, service discovery, staging and rolling releases etc.), observability (metrics, healthchecks, distributed tracing, access logs) and security (mostly certs and authorization, for ex. JWT access rules on service-level); also it would be useful to compare Envoy sidecar proxy Istio utilizes on service-level with kube-proxy in Kubernetes that works on node-level; briefly descrive data plane and control pane in Istio.
L|Kubernetes Ingress vs alternatives (Traefik, HAProxy, Istio Ingress)|Engineering/Distributed|H|Compare popular Kubernetes Ingress alternatives and write few bullet points when it's worth switching to one of them.
M|Domain (private) events vs integration (public) events|Architecture/General|L|Compare domain and integration events along with use cases;domain events - internal communication in-memory between aggregates in same bounded context/service boundry; integration events - out-of-process for external services, modules or even systems that are meant to do something with it in async manner
M|Working set vs private set|Engineering/Operating systems|M|Describe differences between private set (aka private bytes), ie. memory that process allocated (and is in use or paged out) and working set, ie. memory, that process is actively using (in main memory). There are also virtual bytes, which stand for total virtual adress space used by process. This metrics can be helpful in troubleshooting OutOfMemory exceptions caused by memory leaks (private bytes > working set), memory fragmentation (virtual bytes > private bytes) or simply when allocating more memory, than can fit in RAM + pagefile.
L|SQL Server - Instance vs VM stacking|Engineering/Databases|M|Describe differences (in cost, licensing, administrating and troubleshooting) for provisioning SQL Server databases with instance stacking (multiple instances on one VM) approach vs VM stacking (single instance per VM) approach. From my current notes it's worth analysing SQL Server and Windows Server licensing (per core vs per installation), resource sharing, maintainability (when multiple instances can have impact on each other on same VM) and so on.
H|SQL Server - full, differential and transaction log backups|Engineering/Databases|H|Describe different types of backups, that SQL Server offers: full (`.bak`), differential (`.bak`) and transaction log (`.trn`). First of all, it would be essential to compare recovery models: `SIMPLE` (full and diff only) and `FULL` (full, diff and transcation log). Then, it would be worth describing why SQL Server has two (actually three but hey...) files behind the scenes: `.mdf` (Master Database File), that stores data and `.ldf` (Log Database File), that stores transactions, ie. changes to database. When change happens in database, it gets changed in-memory (in a buffer) and modified data is marked as dirty (dirty pages) and change is logged to transaction log in .ldf file on disk. Later on this change will be finally persisted to .mdf file asynchronously on so called `CHECKPOINT`. The reason why three types of backups exist is to prevent data loss and to decrease recovery time when disaster occurs. Full backup backups literally everything, that is currently in database. Differential backup consists of everything that changed since last full backup, so if full backup was on 1st day of month and you created differential each day, on 10th day of month you will have 10 backups, where 10th backup has all the 10 days (1d-10d) since full backup, 9th has 9 days (1d-9d) of changes sinces last backup and so on. Since differential backups (not to mention full backups) take time, transaction log backups are performed much more frequently (ie. per hour, per 15min - dependes od disaster recovery model in organization). The reason why we won't simply do full backup each X days + all of the transaction log backups is recovery time. In such model we would have to restore each transaction log in order, sequentially one after another after full backup restore. If we take transaction log backup per 15min, we would have to apply 96 .trn files after 1 .bak file to recover database from a backup day ago, instead of at most 16 .trn files, if we have differential backup every 4 hours. Why don't we give up on transaction log backups though (that's what SIMPLE recovery model actually is)? First of all, we can't practically make frequent diferrential backups on larger/busy databases effectively. Also, if we recall how data is actually written from .ldf to .mdf file, we can think of a scenario, when we lose data, that was not yet written to .mdf file (but that could be somehow mitigated with `recovery interval` settings on server to issue frequent checkpoints at cost of busy I/O).
M|Non-cryptographic hash functions (Murmur, FNV, SipHash)|Engineering/Computer science|M|Describe usage of non-cryptographic hash functions. Common functions are `Murmur`, `FNV` (Fowler-Noll-Vo) and `SipHash`; there is also `xxHash`, which claims to ran at close to RAM speed(!). Historically, most popular non-crypto hash function was `CRC32` (which is probably still in use, for example for data integrity check of zipped files). Since these hash functions are not (generally) tweaked to be used in security scenarios, they tend to be faster than cryptographic functions. They're usage nowadays is for data integrity checks (CRC and checksums), verifying uniqueness and for hash tables implementation (Murmur3 is used in memcached, Redis and ElasticSearch). Interesting comparison can be found o StackExchange: https://softwareengineering.stackexchange.com/questions/49550/which-hashing-algorithm-is-best-for-uniqueness-and-speed/145633#145633
M|Cache stampede|Engineering/Distributed|L|Cache stampede is a situation, when under heavy load multiple processes attempt to compute resource (for ex. web page) to cache upon cache miss. This can possibly lead to situation, when resource, that we attempted to cache will never be served to user due to timeout. This can also lead to general system instability to due to resource (CPU, network or IO) starvation. Possible mitigation can be locking or delegating resource-to-be-cached computation outside of the process (assuming it will be single place, it will know of all incoming requests to cache such resource).
M|Law of Demeter|Engineering/Design patterns|L|Law of Demeter (*"don't talk to strangers"*) is a rule, that states, that class C should talk (method call) only to it's own members, objects created by it, arguments passed to its methods and to global (static) objects. Therefore it forbids calls like `person.getAddress().getZipCode()`. Ofc in general getters are not an issue - actual business operations performed on objects other than "neighbours" are the issue here. They lead to coupling between objects, that is not so obvious when analysing given class.
H|.NET performance troubleshooting|Languages/C#|M|Describe commonly used tools to troubleshoot .NET applications performance. The ones that I've mostly used are *PerfView*, *DebugDiagTool*, *perfmon* with choses performance counters (depends on your usecase), *Visual Studio profiling session* (amount of calls and CPU time, managed heap structure etc.). There are also new tools in .NET CLI, especially useful on Linux environments, from which most of can be found on dotnet organisation on GitHub in diagnostics project (mainly `dotnet-counters`, `dotnet-dump` and `dotnet-gcdump`).
M|SQL Server - `WITH(NOLOCK)` vs `WITH(READPAST)`|Languages/SQL|L|Compare `NOLOCK` (== `READ UNCOMMITTED`) with `READPAST` hints in SQL Server. Both of these hints are used for speed optimisations, but they differ in a fact, that `NOLOCK` reads state of records, that are currently inserted/updated/deleted in a pending transactions (so *dirty read* occurs) wheras `READPAST` in a same situation will skip locked rows.
M|SQL Server - RCSI (Read Committed Snapshot Isolation)|Engineering/Databases|M|Describe how `RCSI` mode works in SQL Server. RCSI is used to achieve optimistic concurrency model in SQL Server by using version store in **tempdb**. All the current transactions running under (generally default) `READ COMMITTED` will now skip locking on read enitrely and will receive last committed version of read record from the moment of when transaction started. This does not changes how other isolation levels work (which is sometimes forgotten, by myself for example...). RCSI in most cases will increace concurrency of the database but will come at a cost of higher tempdb utilization (which should be closely monitored and perhaps some upgrades of this part of SQL Server setup should be planned). RCSI is often summarized as **writers do not block readers (and vice versa)**.
M|Indexed view (materialized view in SQL Server)|Engineering/Databases|M|**Indexed view** is a SQL Server implementation of materialized view. To create indexed view you need to create simple `VIEW` with `WITH SCHEMABINDING` option and `UNIQUE CLUSTERED INDEX`. Indexed view (as any other materialized view implementations) stores data physically, just like tables do (in contrary to classic views). When querying against remember to use `WITH(NOEXPAND)` hint on SQL Server versions lower than Enterprise. It will force query optimizer to use actually use indexed view instead of using table behind it. When moving from backlog, compare indexed view with Oracle SQL and PostgreSQL implementations of `MATERIALIZED VIEW`.
L|`WITH SCHEMABINDING` options|Engineering/Databases|M|Why use `WITH SCHEMABINDING` option in SQL Server? I've found two reasons so far: first of all, if we want to avoid accidental breaking of views when source table is changed, schema binding helps, because it forces DBA that alters the table to carefully review changes schema-bound view too (because view will have to be recreated). Second considerations is for UDFs that compute scalar values - when we use schema binding on them, we let know query optimizer to analyze body of the function and to possibly skip *Halloween protection* measures (like Table Spool) when it's not really needed (boosting performance as a result).
M|`CancellationToken` in ASP.NET Core|Languages/C#|L|Automatic binding of `CancellationToken` as a parameter to controller action in ASP.NET Core allows some interesting techniques in typical OLTP systems. When a request is aborted (for ex. user aborts request before it finishes), cancellation is requested on CancellationToken bounded to called controller action. While this might be not so useful on Controller level, we could pass it further, for ex. to Query/CommandHandler `Handle()` parameter and introduce some sort of transaction rollback (or skipping `SaveChanges` on `DbContext` or skipping `Complete` on `TransactionScope`), when we decide to commit, but find out that user actually aborted request. This might seem more natural to some of the processes, where user expected action to be aborted but modifications were still applied to the system.
M|`async/await` behind the scenes|Languages/C#|M|Describe how `async/await` works behind the scenes, what is cost of it (generating compile-time state machine and running it with queuing and scheduling continuation execution). It would be worth mentioning how state machine behind the scenes looks like, what optimisations are made for already completed tasks and why state machine is `class` in Debug but `struct` in Release. 
H|Optimistic concurrency control|Engineering/Databases|M|**Optimistic concurrency control** (OCC) is a way of guaranteeing correct results  when concurrently modifying state in database/filesystem etc. without locking the resource like database record (which would be a **pessimistic concurrency control**). It has its advantages (no locking, so no deadlocks and no need to manage them) but also introduce some challenges (managing versions in storage, passing them to user and back, introducing version mismatch strategies, ie. rollback or retry with user consent etc.). In typical HTTP web application, version of the record is sent to user using `ETag` HTTP header. Many ORMs include OCC out-of-the-box. For example `EF Core` supports it with timestamp-based **ConcurrencyToken** (either with data annotation `[Timestamp]` or with `IsRowVersion()` in fluent API). General strategy is to compare concurrency token on modifying state (`UPDATE` for example) in `WHERE` clause, ie. `UPDATE my_table SET xyz = 123 WHERE id = @id AND concurrency_token = @token`. If `affected records != expected records` (for single update: 0 != 1) concurrency control error should be raised. Optimistic concurrency can not only be used in classic `user -> web app -> db` scenario, but also help in deadling with distributed state in service-service (especially async) communication, when mismatched versions on requested operation should lead to message rejection or compensation (or both).
H|GRASP (General Responsibility Assignment Software Patterns)|Engineering/Design patterns|M|**GRASP** is a set of nine fundamental principles in object design and responsibility assignment, first applied by Craig Larman in *Applying UML and Patterns*. <br> Those principles are (P - problem, S - solution):<br> `Information Expert` (P: Where do I assign responsibilities? S: To objects, that has data to fulfill them),<br> `Creator` (P: Who creates objects A? S: Object of type B, if B aggregates A, B records instances of A, B closely uses A or B has required information to instantiate A),<br> `Controller` (P: Who should be responsible for handling input? S: Controller dedicated to given use case or group of related uses cases),<br> `Indirection` (P: Where and how to assign responsibility, to avoid direct coupling, while keeping high reusability? S: Introduce indirection through additional object responsible for mediation),<br> `Low coupling` (P: How to decrease impact when changing one class to other classes? S: Decrease direct dependencies between objects.), <br> `High cohesion` (P: How to keep objects focused, maintainable and understandable? S: Object should keep only those responsibilities, that are highly related to each other.),<br> `Polymorphism` (P: How to make components pluggable or replacable with alternative behaviour? S: Identify points of possible instability ie. alternative behaviour and introduce interfaces, instead of concrete implementations there),<br> `Pure fabrication` (P: How do we achieve rules like indirection without polluting problem domain? S: We can introduce intermediate classes, that do not represent problem domain per se, for ex. domain services in Domain Driven Design).<br>
M|Cohesion (programming)|Engineering/Design patterns|M|Cohesion in programming is a measure of determining *to which degree elements inside a module belong together*. When we mean that class has high cohesion, it means that its methods have a lot in common and do not process lot of unrelated data. Types of cohesion are as follow (from lowest/worst cohesion to best/highest cohesion):<br> `Coincidental` - parts arbitrarily groupped together in a same source file but not really common (*Utility* classes etc.)<br> `Logical` - parts groupped by type of logic they represent, for ex. all the *Controllers* in MVC pattern in same folder (logic - Controller, but different use cases/functionalities)<br> `Temporal` - parts groupped by moment in time in execution of program, when they are processed (ie. functionalities run on startup groupped together fit in here)<br> `Procedural` - parts groupped by sequence of execution (before X do Y)<br> `Communicational/information` - parts groupped because they operate on same data<br> `Sequential` - parts groupped, because output from operation X is input to operation Y (so opposed to procedural cohesion, this one is more of a neccessity); an example would be - I guess - companion methods to template method pattern?<br> `Functional` - parts groupped because they all contribute to the same, single task
M|Bus factor|Soft skills/Project management|L|Bus factor is a risk management measurement, that helps to determine single points of failure when key technical expert is gone and information/capabilities that person had, have not been shared (that's what name comes from: *what if person X get hit by bus?*). Bus factor is calculated by person count that would need to disappear from process/project/etc. to for it to stall. For ex. if in IT project single person has knowledge how compoent X works and such knowledge has not been shared (and is currently hard to catch up), we can say that bus factor = 1. If we'd have three person with such competences, bus factor would equal 3.
H|Index fragmentation|Engineering/Databases|M|First of all let's recall how SQL Server organizes data: into 8KB `pages` and then into 8-page `extents` (64KB total). <br>**Index fragmentation** is a situation that occurs when logical order of pages used by an index does not correspond to physical order of those pages. What causes it? In general `INSERT`, `UPDATE` and `DELETE` operations cause it, since when data do not fit into 8KB page, the page gets split along with (in general) half it going to new page. But why would it even cause performance problems? Well if pages are contiguous on disk, SQL Server can read-ahead up to 64 (512KB) of them at once (sequentional read); when they are "scattered" around, SQL Server needs to perform more I/O operations (random access read) leading to highers resource use. That situation is also called **logical fragmentation**.<br> To avoid index fragmentation affecting perfromance of DB, we can schedule or execute ad-hoc two operations:<br> - `ALTER INDEX IX_my_index ON mytable REBUILD` - rebuilds index from scratch; it's an offline operation that locks out table during the process (if not using `ONLINE` option on Enterprise Edition); it's multithreaded and uses lots of I/O; statistics are updated after it finishes. It's all or nothing operation.<br> - `ALTER INDEX IX_my_index ON mytable REORGANIZE` - squishes rows together and organizes the pages for physical order to match logical one ; reorganizing index does not lock it out (it's an online operation); it is singlethreaded but has low disk footprint (only 8KB - single temp page)<br> <br>Another type of fragmentation is **internal fragmentation** which stands for situation, when page density is low (and so we have too many pages when compared to amount of data we store). In general its cause is low `FILLFACTOR` setting; if we have low page fill/density, SQL Server needs to jump around the pages to get data, since it doesn't get it from fetching single page (it did not fit on it).
M|Angular change-tracking|Languages/TypeScript|M|To implement binding and updating views, Angular implements change-tracking mechanics behind the scenes. For anything that happens in the browser, change tracking is implemented using Zone.js by *"monkey patching"* browser APIs, ie. `add/removeEventListener` functions, `Promise` APIs, `setTimeout` and `setInterval`, `XMLHttpRequest` aka XHR (AJAX calls) and more. For component state bound to views, Angular uses 3 change tracking strategies: `Default` (tracks fields of mutable object), `OnPush` (tracks only references, works best with immutable objects; works also with `async` pipe) and `disabled` (for manual firing change detection cycle). How detection mechanics (in `Default` strategy) actually know which fields to track? Angular analyzes syntax of component's template, find bindings and keep track of only those, which values changed (by comparing previous and current value of course).
M|Deferred vs immediate domain events raising|Architecture/General|L|There are two approaches/patterns to raise domain events in DDD. First one (I guess an older one) has been proposed by Udi Dahan and is a simple `EventRoot` static class that receives events, subscribes handlers and orchestrates forwarding events. I actually did this one as a Kata in devs-pl initiative here: https://github.com/devspl/Kata---Business-Rules/tree/master/m-wilczynski-csharp-payment_handlers There is also a second approach, proposed by (I think) Jimmy Boggard that we put our domain events into collection of events which is property of our domain object (so `IReadOnlyCollection<IDomainEvent> MyDomainObject.DomainEvents`). We then collect them on our handler/use-case level and publish them before or after commit (that's an architecture choice to be made here). Second approach makes it also much easier to unit test event flow, since we can use "*Chicago school*" (aka state school) of unit testing and assert state of an object (ie. collection of events) after we acted upon it (ie. did some business operation).
L|SQL CMD checking with `NOEXEC`|Languages/SQL|L|When writing scripts in T-SQL with SQL CMD mode on, you might want to check whether script has all the variables properly set and if SQL CMD mode is set properly. To prevent running some of the code or to validate it we can add `SET NOEXEC ON` to make SQL Server parse it, compile it but not execute it. To leave compile option out too we can use `PARSEONLY` instead.
H|Top 10 OWASP|Engineering/Security|M|**Top 10 OWASP** is a document from organization called Open Web Application Security Project, pointing most common security issues in applications. <br>These issues are:<br>- **Injection** (like SQL injection - placing untrusted data as a part of a query; vulnerable application would be the one, that does not sanitize user inputs before executing queries, leading to easy access to critical - from security POV - queries like `SELECT * FROM users`), <br>- **Broken auth** (weak session, password and auth management, ie. does not implement MFA, does not update max. length and complexity of passwords, does not check passwords against top X common passwords, does not delay failed logins to to make brute force attack impractical etc.) <br>- **Sensitive Data Exposure** (not encrypting data at transit and at rest aka used"plain text"; using old crypto algorithms; caching sensitive data; using weak hashes, not using salt etc.) <br>- **XML External Entities** (by default older XML processors would process external entities by evaluating them on processing; these could lead to Denial of Service vulnerabilities or probing private network resources for protocols like SOAP < 1.2 and SAML) <br>- **Broken Access Control** (not enforcing granular permissions or even not checking what user can and cannot do in a system, that may lead to performing unauthorized actions and/or accessing confidential data) <br>- **Security Configuration** (not setting up properly or at all access control for resources; not setting security-oriented HTTP headers, more: https://owasp.org/www-project-secure-headers/; not patching OSes, runtimes, frameworks and libraries in timely fashion) <br>- **Cross-site scripting (XSS)** (accepting untrusted/non-sanitized input from users, that is later on used to render other users' views, leading to execution of malicious scripts on client-side) <br>- **Insecure Deserialization** (not performing strict checks of incoming serialized data for non-primitive types, so binary serialization other than JSON etc.; OWASP mentions user session state deserialization, that has no integrity checks with means like signature check in JWT) <br>- **Using Components with Known Vulnerabilities** (common problem in large applications and/or organisations, where tracing every possible used library or framework and tracking all of the CVEs (Common Vulberabilities and Exposures) is hard, due to the complexity of the environment) <br>- **Insufficient logging and monitoring** (no matter how thoroughly you've checked your code, if you have no proper logging and monitoring to identify suspicious actions and no process to quickly escalate security exceptions - you'll fail)
M|Unit testing schools|Engineering/Testing|M|I've heared of **schools of unit testing (or TDD schools)** from some time, so I've decided to take a deeper look into it. It seems that there are two of them:<br> - **Chicago (also called Detroit) school of unit testing aka "State-based" aka "Classicist" aka "black-box"**: focuses on state-based testing, so if we perform operation `A` on `MyObject` (literally `MyObject.A()`), our assertions should be against the state of `MyObject` or at least about the result of the operation that we retrieved. <br> - **London school of unit testing aka "Mockists" aka "white-box"**: focuses on interactions between objects and assert the usage of dependencies and reaction to what they return. <br> I would say both schools have their points and should be used interchangeably. I'm more and more leaning towards state-based approach, but saying that I would not to double-check on how we handle (especially external) dependencies would be an understatement.
H|SQL Server - Statistics|Engineering/Databases|M|Every developer dealing with more than basic usage of T-SQL has gone to the point of at least acknowledging, that assumptions upon queried data that query optimizer makes are based on statistical data SQL engine collects called ***Statistics***. But what Statistics consist of? Statistics are BLOBs that hold data (of 1 or more columns) about distribution of values in a table (in a form of `histogram`), which allows query optimizer to estimate number of rows and to use best matching operator. Statistics also hold data about correlation of values in a table called `density`, which is based on number of distinct values in a column. Statistics can be viewed by either SSMS (under table element) or by T-SQL. <br> There are two strategies on maintaining good statistics and both should be employed: <br> - `AUTO_UPDATE_STATISTICS` (database option) - query optimizer determines whether statistics should be updated before executing query. Engine uses two factors to determine update (aka recompilation): `# of modifications since last recompilation` and `table cardinality`  (uniqueness of data values that table holds; high cardinality -> lot of distinct values). Auto update can be done synchronously (so query will wait for statistics update but will benefit for up-to-date statistics) or asynchronously with `AUTO_UPDATE_STATISTICS_ASYNC ON` (query will not wait for new statistics and will not benefit from them but will trigger the process of updating, so next queries will benefit) <br> - `Monitoring and manually updating statistics when needed` - when queries are performing badly and operators in query plan suggest unsufficient or lacking data for query optimizer to make right decisions, there might be a need for manual update with `UPDATE STATISTICS my_schema.my_table;` (or even more granular with certain table statistic) or going all-in with `sp_updatestats` to perform full database statistics update (which is risky, given how much resources it would take)
M|Performance counters for IIS-based web server|Enigneering/Operating systems|M|Watching your web server health is an obvious thing to do and on Windows Server based servers we can do so with performance counters. Note - this won't tell you much about ASP.NET Core apps performance, since they do not natively use Performance Counters at all. See `dotnet-counters` instead. <br>  PerfCounters that could prove to be useful are: <br> - `Memory - Available MBytes` - available RAM <br> - `Processor - % Processor Time` - CPU time, especially useful when comparing particular w3wp.exe process  utilization vs total system utilization <br> - `Virtual bytes` and `Private bytes` per w3wp.exe - can help pin pointing possible memory leaks and/or memory fragmentation issues <br> - `HTTP Service Request Queues - CurrentQueueSize` for HTTP.sys kernel side queue - requests waiting for IIS to pick them up <br> - `.NET CLR Exceptions` - for, well - exceptions thrown in total <br> - `Request in Application Queue` - ASP.NET (or rather CLR Thread pool) queue; sinces it's server-wide statistic, it can only tell if web server is reaching it's bottleneck (and reaching thread starvation I guess?) <br> - `Requests/Sec` and `Requests failed` per ASP.NET application can tell a little more about throughput and if there is no spike in failed requests (that could indicate problems with an app)
L|Implicit, explicit and auto-commit transactions|Engineering/Databases|L|Implicit, explicit and auto-commit modes for transactions are ways of RDBMS supporting (or not) wrapping statements in transactions. Default behaviour in SQL Server is auto-commit, which means that when we execute a statement, engine automatically starts transaction, wraps it around the statement and finally commits it (so we don't manage transaction at all). When we use implicit mode, engine only starts transaction and wraps our statement with it but does not commits it automatically. When using explicit mode, we are responsible for all the steps: `BEGIN TRAN` and either `COMMIT` or `ROLLBACK` of it.
M|Data compression - SQL Server|Engineering/Databases|M|Compression is something we come up with with when we want to save up some space. We can compress tables or indexes with `ALTER TABLE/INDEX dbo.index_or_table REBUILD WITH ( DATA_COMPRESSION = ROW/PAGE )`. What I found is more interesting - it seems that compression can actually positively affect database performance. In general compressing data saves disk space but uses more CPU, since data has to be compressed on writes and decompressed on reads. However, many blog articles suggest, that if I/O and/or memory pressure is an issue on your SQL Server instance, perhaps sacrificing a little of CPU can be worth trying out. It's especially recommended for read-heavy tables (citation and general evidence needed).
H|DDD patterns cheatsheet|Architecture/General|H|I've used bits of both strategic and tactical patterns from DDD for years now, but never have I took time to wrap them up in a quick, lookup cheatsheet. So, it would be worth writing a little (along with some personal thoughts) about: <br> - strategic patterns - "whole" `domain` discovering (users, requirements, usages, constraints) and its `ubiquitous language`, `bounded context`, `core domain` and `generic and supporting subdomains` that result from domain distillation along with all of the `context mapping` strategies, <br> - tactical patterns - `aggregates` as transcation/consistency boundries (along with basic design principles for building them, perhaps along with `entity`, `value object` and `aggregate root` as building blocks) and  `domain events` (I have both public vs private event differentiation and handling patterns topics already queued in backlog); <br> There's also a super-awesome starter kit from ddd-crew that I've forked that wraps it all really nicely: https://github.com/m-wilczynski/ddd-starter-modelling-process
M|EventStorming cheatsheet|Architecture/Business analysis|M|EventStorming as a "discovery technique" have proven to be extremely useful to my team in a last project. EventStorming session is performed on a timeline-like (flow of time goes right way) board with sticky notes as a collaboration tool. Heart of our EventStorming session would be identification of `domain events` (orange note) - concept from DDD (that doesn't neccessary need to result in pure DDD-solution on implementation stage). Domain events are also our starting point for first phase of our discovery session, where we would try to grasp high-level view on a business procesess in general (also called *Big Picture EventStorming*). After (hopefully) getting a big picture perspective and identifying subdomains of our problem space, we would move to the second phase - *Design Level EventStorming* (that sometimes gets splitted in an additional, pre-design phase of *Process Level EventStorming*). On this level we'd add `actors` (dark yellow note) that execute `commands` (blue note) upon `aggregate` (light yellow note) or generally on a `system` (pink note). Both command execution and its result are presented on a `view/GUI` (green note). There might by critical areas in a processes called `hotspots` (red, rotated 45deg note). Optionally, we can also add immediate (sync) enforcements - `rule/validation` (light blue) and async (as a reaction to domain event) `policies` (purple note).
L|Inlining hints for JIT Compiler|Languages/C#|M|Inlining is a process of replacing method call with method body if neccessary (ie. performance boost could be worth the duplication). In general, most obvious cases will be inlined anyway by compiler without any hints provided. This however might not be true for less obvious cases or for a hotspots, where compiler might not recognize the value, that inlining could provide. Or sometimes compiler does not have a strategy in place for such case. That's when `[MethodImpl(...)]` attribute comes to the rescue providing hints for compiler where inlining should take place (note: "should" != "will"). Mentioned attribute accepts flags of enum `MethodImplOptions` and those, that we should be interested in for this case are `NoInlining` (8) and `AggressiveInlining` (256). There are also few other interesting things in MethodImplOptions, with notable mention of `Synchronized`, that enforces lock on any caller of annotated method.
M|Data cardinality|Engineering/Databases|M|**Cardinality in SQL querying** (as a part of statistics) is a measure of how unique or distinct values are in a column. Low cardinality means lots of same values, while high cardinality means most of (or all) values are unique (think primary keys, GUID/UUID etc.). This metric is further used for query optimizer to make decisions on what query plan it should use, as it partially affects things like query selectivity (needs additonal note). <br> But (!) there is also a term of **cardinality in monitoring**. Consider monitoring data as a set of `metric_name` - `dimensions` - `metric_value`, we would say that high cardinality of monitoring data means that any of the metric dimensions has lots of distinct values (so actually in definition pretty close to SQL terms). Let's give an example: if we have metric of cpu.utilization: `{metric_name: cpu.utilization} {dimensions:[{'server': srv01}]} {metric_value: 0.7}`, then if we have 200 servers that we monitor CPU on, each one of them would have different value of `dimensions.server` and would mean high cardinality. It's important, since if we want to query data by dimension, it needs a good indexing strategy. If we have 200 distinct servers, that's not that bad (but still 200 servers x heartbeat per 5s everyday). But if we add another dimension (for example `service_name`), we now have cartesian product of both dimensions, exploding number of series by an order of magnitude.  <br><br> Wrapping up - for SQL querying high cardinality is good - if column used for query has high cardinality, then using it in WHERE clause gives us high selectivity.<br> For monitoring high cardinality is bad, because it leads to higher number of time series, affecting time series database performance due to higher resources requirements to serve user queries.
L|String interning|Languages/C#|M|String interning is a technique used by .NET runtime to store and reuse string literals (so "hardcoded" strings) instead of reallocating them over and over again. So if we have somewhere in code literal like `var myPrefix = "PREFIX_"` and in some other part of the code we have the same literal declared, we would not get newly allocated string with same value, but we'd receive the same tring with the same address in a memory (so no copying and allocating involved). <br>Behind the scenes, interned strings are stored as any other string - on SOH or LOH depending on their size. Their adresses are stored on special structure called **String Intern Pool** (placed on LOH) and further "registered" in a *String Literal Map* hash table (on .NET Framework unmanaged private heap). This hash table is looked up on any attempt to intern string and if such string is already interned, existing interned string address will be returned instead.
L|`ArraySegment<T>` vs `Span<T>`|Languages/C#|L|`ArraySegment<T>` struct has been introduced very long ago - in .NET 2.0, but actually was quite rarely used (in my opinion). Its use is to provide a "sliced, mutable view" over a part of an array without actually allocating new array. It became much more useful since .NET 4.5, when it started to implement `IEnumerable<T>`, `ICollection<T>`, `IList<T>` and `IReadOnlyCollection<T>` (all the LINQ goodies and so on). `Span<T>` ref struct is a much broader (or rather general) implementation of "mutable view" on part (slice?) of a data (actually - a contigous region of memory) that has been allocated on either managed or unmanaged heap or on stack with `stackalloc`. Span is always allocated on stack - there is no way to allocate in on heap (it's not allowed to be field of a reference type that would make classic struct to be placed on heap, boxing of it is not allowed etc.). For a "read-only view" over a contigous region of memory use `ReadOnlySpan<T>`.
M|Redis transactions|Engineering/Databases|M|Although **Redis** is usually used as a plain-old cache solution, it doesn't mean it has no more advanced features. One of them are transactions, which are implemented in a little less "complicated" way than those in SQL engines. Redis offers `MULTI` command as a way to start a transaction and `EXEC` as an execution of all of the queued commands after `MULTI` command. `EXEC` executes all of the commands sequentially and guarantees, that no other client will perform any operation in-between. Note: all of the commands are executed the moment `EXEC` is fired! <br> Also, there is a way to abort transaction with a `DISCARD` operation (which essentially dequeues previously queued commands).<br> What's more interesting is that Redis also supports optimistic concurrency via `WATCH` command. `WATCH` is essentially a "check-and-set" operation that tracks given key and commits transaction via `EXEC` only if watched key did not change in a meantime.
M|TDD vs BDD|Engineering/Testing|M|Since I've made multiple (sometimes more, sometimes less successful) attempts to incorporate TDD to my everyday workflow, I've found myself walking the (in my opinion) same worthless path of testing every damn thing, neglecting the real value tests should offer to business stakeholders. The quality of tests have of course increased with TDD a lot, since implementation does not have to take testing "into considerations" - it's builtin. However, I kept on missing the business value that those tests could present so not so long ago we've introduced annotations to our Selenium tests with `Fody`, to record every step that happened throughout the test execution. I liked it a lot, since we had "test scenarios describer" built-in, but that was still a "post-factum" addition to the test, instead of being a starting point for it. That's where BDD came into my mind, as I've read about it a long time ago and I guess now pieces start to come together. Provided we have specification in place (for ex. as a formalization of discovery stage), we could easily provide lots of human-readable `Given-When-Then` scenarios (in Gherkin or anything that fits), that could be a "common-ground" for developers, QAs and business (like Product Owners) to talk about during the development process.<br> So, to put TDD and BDD in comparison, I find TDD as a development technique, wheras BDD is an evolution of TDD that pushes it on "higher-level", making it a team/development process practice.
M|Edge vs fog vs cloud computing|Architecture/Distributed|L|When diving deeper in a world of cloud computing I've figured out another "types of computing" that gain traction recently. One of them is (I guess more popular) `edge computing` and another one is `fog computing`. While cloud computing offers an almost unlimited scaling, storage etc. (and lots of other cool things like infra-as-a-code, managed services like k8s, identity providers etc.), it's often inefficient to send all of our data to cloud at once. That's where edge computing cames into action, offloading part of the computing and preprocessing to the edge servers (servers near our collected data, so for ex. near IoT sensors). To further improve latency, reduce storage usage in the cloud (think costs) and perhaps filter out some of the not needed data, we can introduce concept of fog computing - to have smaller data centers between edge servers and the cloud, that would be capable of receiving data from edge layer and forward them to cloud only if needed and for ex. only on give part of day (to better utilize resources in the cloud etc.). Since fog layer is meant to be closer to edge layer, latency would be much lower than than connecting directly to the cloud, so sending data from things like IoT sensors can be done immediately, instead of batching etc.
M|Storing JSON in SQL Server|Languages/SQL|M|There are two ways for storing JSON in SQL Server. If you plan to use JSON querying (and other features) available on 2016 version and onwards (or on Azure SQL), you should use `NVARCHAR(MAX)`. However, if you only intend to store JSON data in database but not really intend to query for values inside it, you can store it in `VARBINARY(MAX)` field. What's even more interesting, is that we can use `COMPRESS(<string>)` function on inserting JSON value to the VARBINARY column (to better utilize our storage) and create another, computed column `json_as_text AS CAST(DECOMPRESS(json_as_binary) AS NVARCHAR(MAX))` that will decompress and convert JSON to text representation, effectively offloading (and simplifying) consumption on application side (at a cost of CPU on database side).