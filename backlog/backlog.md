# Backlog

|Priority (L/M/H)|Topic          |Category       |Effort (L/M/H)|Collected notes|
------------- |-------------| ---- | --- | --- 
H|Merge 'Languages' and 'Frameworks' sections| - |M| - 
L|WCF connection patterns|Languages/C#|M|`Dispose` vs `Close`+`Abort`, proper use `ChannelFactory<T>`, caching (what and why)
L|SCIM2|Architecture/General|L|SCIM2 is an API open standard for cross domain identity management
L|PIM/PAM|Architecture/General|M|Class of solutions that manage elevated permission accounts and their sessions (ie. admins, super-users and so on)
M|CMM|Architecture/General|M|*Capability Maturity Model* - development model for assessing how mature particular processes are in certain organization/department/project; this can span from things like DevOps maturity, cloud-readiness, quality gates, code-review, self-organisation etc; measurement ("scoring") spans from 1 (Initial) to 5 (Effective)
H|SQL Server locking debugging|Languages/SQL|L|Expand current *T-SQL Performance Analysis Cheatsheet* with `sp_lock` and `sp_who` + SPID
M|XAdES|Engineering/Security|H|XML signing - expand on how it actually works "underneath"; used for **qualified electronic signature** (recommended by UE and MSWiA in Poland)
H|SQL Server - `UPDLOCK` vs `SERIALIZABLE`|Languages/SQL|M|`UPDLOCK` hint can be used instead of `SERIALIZABLE` isolation level to prevent duplicates `INSERT` in (`SELECT` then `INSERT` scenario); it can gracefully force other sessions (that also use this hint) to wait for their "chance" to SELECT without ending up with deadlocks (as in `SERIALIZABLE` case)
M|SQL Server - lock compatibility matrix|Languages/SQL|M|Describe compatibility matrix between Shared (S), Exclusive (X), Update (U) and all the Intent types (I...)
M|SQL Server - `RoundTimeDown`|Languages/SQL|L|Describe creating time buckets in SQL Server with `RoundTimeDown`; quite handy for reporting and all time series oriented data in SQL Server
M|OpenTelemetry|Architecture/General|H|Describe OpenTelemetry - it's goals, current state and usage; from their page it `is a collection of tools, APIs, and SDKs. You can use it to instrument, generate, collect, and export telemetry data (metrics, logs, and traces) for analysis in order to understand your software's performance and behavior.`
M|Microfrontends|Architecture/Distributed|H|Describe Microfrontends as a general approach to decomposing monolithic frontends and as a final step in *vertical slices* approach to succeed
L|Webpack Module Federation vs single-spa|Languages/JavaScript|H|Describe differences between these two approaches and how they can be used to achieve microfrontends when needed
M|TailwindCSS|Languages/CSS|H|TailwindCSS has been gathering large popularity for a long time already; investigate and write down in your words how it works and "why the hype"
L|StencilJS|Languages/JS|H|StencilJS has been recommended in many places as a go-to solution for Web Components (and building design systems with them); investigate and write down how in general it works
L|RBAC,ABAC,ACL|Architecture/General|M|Briefly desribe RBAC, ABAC (RBAC + attributes) and RBAC (business role - create, write, enslist, pay etc.) vs ACL (technical - has or has not access to resource)
M|Marshalling|Engineering/Distributed computing|M|Briefly desribe definition of marshalling and find few examples of it like RPC/REST (with JSON/XML/MessagePack etc.), COM etc; compare with serialization
M|x86 assembly basics|Languages/Assembly|H|"Serialize" notes from reading ***C.O.D.E.***; note down popular x86 opcodes to be not so ignorant when reading x86 ASM files
H|How computer works? - high level|Engineering/Computer Science|H|Create simple cheatsheet (with diagrams) on how computer architecture works, ie. CPU (ALU, registers and L1, L2, L3 cache), memory, disc, bus speed and ideas/problems around it, ie. cache misses, branch prediction, register spilling (moving to RAM) etc.
L|Shift left|Architecture/General|L|"Shift left" is an organisation approach to security, that encourages security review as early in development process as possible (so "left" means literally left on time axis given software development lifetime)
L|Sonar vs Fortify|Standalone software/CICD|H|Both tools are used for static code analysis; Sonar is used for for code quality analysis (and measuring technological debt), whereas Fortify is used for code security analysis; compare how they can be used together in CI/CD pipeline
H|Certificates in PKCS|Engineering/Security|H|Describe what role certificates play in Public Key Cryptography Standards (PKCS) infrastructure; describe what thumbrint (hashed certificate), signature, CA (Certificate Authority) and trust chain is; throwing in few words about asymmetric cryptography wouldn't hurt
M|Array pooling with `ArrayPool<T>`|Languages/C#|M|*Array pooling* is a concept of initializing large array beforehand (if we anticipate that we will be allocating large arrays and do it a lot) and then renting chunks from it when we need them; if we keep renting from once preallocated pool, we do not end up with lots large arrays on LOH (Large Object Heap), that can later on cause forced managed heap collection (which hurts performance a lot); `ArrayPool<T>` offers pooling and renting (with `ArrayPool<T>.Rent`) functionality in .NET and is thread-safe; Adam Sitnik has amazing post about it on https://adamsitnik.com/Array-Pool/
H|Small Object Heap (G0, G1, G2) and Large Object Heap|Languages/C#|H|Write down general mechanism behind .NET Garbage Collection, SOH (and its generations) and LOH; include topics like LOH compacting, memory fragmentation problem, when object lands on LOH and general LOH problems; definitely approach with Konrad Kokosa's book accompanying you
L|tcpdump on Linux|Engineering/Computer networks|M|Describe usage of tcpdump - network sniffer for Linux; could be worth comparing it briefly with Wireshark
L|TLS termination|Engineering/Computer networks|M|Describe TLS/SSL termination - how it works, why we do this (performance, network appliance, package analysis etc.)
L|FQDN vs hostname|Engineering/Computer networks|L|Hostname could be FQDN if it goes up to the top-level domain
M|Load balancing - L4 vs L7|Engineering/Computer networks|H|Describe how load balancing is implemented nowadays, especially comparing layer 4 and layer 7 balancing; if it ends up as not a technical but more general note, move to Architecture/Distributed
M|HAProxy|Standalone software|M|Play around with HAProxy and note some general concepts behind how it works
H|async-await vs locking|Languages/C#|M|Write down approaches to using async-await with locking (mainly `SemaphoreSlim`, but could also hackaround your way with simple bit flag and `Interlocked.CompareExchange`)
M|Raft algorithm|Engineering/Distributed computing|H|Describe Raft algorithm and how it achieves cluster consensus with distributed log, leader election etc; note actual uses in solutions like Neo4j (+ONgDB) or RabbitMQ (with quorum queues)
L|Byzantine generals problem (Byzantine fault)|Engineering/Computer science|M|Describe Byzantine fault problem and how it affects distributed computing systems
H|Event schema changes - challenges|Architecture/Distributed|M|Describe solutions to adapt to changing event schemas, especially with event sourcing approach, where multiple versions changing over time can't be guaranteed to properly deserialize to strongly-typed event type; solutions I can think of would be *upcasting* along with usage of *weak schema* (for ex. JSON); consider describing *snapshot* creation too
L|OSINT|Architecture/General|M|Describe (from high level perspective) what open-source intelligence approach is and why you should care in terms of information security
M|Business validation - close to input or in core domain|Architecture/General|M|Describe current thoughts on (especially basic, ie. form) validations outside of core business domain with libraries like `FluentValidation` or `MVC Model validation`, that can offload trivial checks like required, min/max length, pattern match etc; discuss challenges of (possibly) missed rule enforcement in domain model, that can lead to breaking invariants and persisting domain model in invalid state 
H|Angular - change tracking strategies|Languages/TypeScript/Angular|H|Describe how change tracking strategies work in Angular: *default* (with "deeper" checking and mutability support), *OnPush* (shallow, preferred for immutable data) and *disabled* (and manualy triggering using `ChangeDetectionRef`); describe briefly how `Zone.js` works with Angular; decribe how change detection mechanism work with templates (`foreach` + every *template binding* + compare previous value of field with current value but **only for those that are used in bindings**)
M|`[ThreadStatic]` vs `ThreadLocal<T>`|Languages/C#|M|Describe diffferences between `[ThreadStatic]` and `ThreadLocal<T>`; both are used for thread exclusive data (so each thread has its own copy), but `[ThreadStatic]` initializes for first thread only whereas `ThreadLocal<T>` initializes for every thread; `ThreadLocal<T>` also implements `IDisposable` (good for some cleanup); for keeping data local only to particular async flow (async-await chaing) you can also use `AsyncLocal<T>`
M|C# 9 records|Languages/C#|M|Describe how `record` - new C# (9.0) language feature - works; in general records have been introduced for implementing (reference) types, that are described by their data (so for ex. DTOs fit in here), offering out-of-the-box value equality. While meant mostly for immutable scenarios (by default they are immutable with auto-properties with `init` only setters), they can also be mutable. Records support nondestructive mutation (more like pure functional) with `with` keyword, that copy source record and apply given mutation on it, ie. `var secondRecord = firstRecord with { MyProp = "new value" }`. Another great productivity shorcut that records provide is a *positional syntax*, which allows us to define data-type in single line, ie: `public record Person(string firstName, string lastName, DateTime birthDate);` would yield encapsulated, immutable reference type with value equality implemented behind the scenes.
S|Angular - compare *dirty*, *touched* and *updateValueAndValidity*|Languages/TypeScript/Angular|M|Compare *dirty*, *touched* and *updateValueAndValidity* in Angular Forms and describe when and why should each of them be used.
H|Describe formal difference between *system* and *business* analysis|Architecture/Business analysis|M|Describe differences between system and business analysis. It seems that definition of business/system *analytic* and business/system *analysis* differ, as system analysis (according to Wikipedia and some other pages) is a subset of business analysis, which deals with translating organisation's needs, models and workflows into IT systems functionalities. On the other hand, it seems that business analytic does not deal directly with system analysis, even though it's a part of business analysis as a whole. Perhaps for practical reasons system analytic's work has been extracted from business analytic's responsibility list to allow division of work (and also specialization).
M|*Service blueprint* technique|Architecture/Business analysis|M|Describe *service blueprint* technique on how to plot a diagram of how particular service (let's take order as an example: order->pay->package->delivery) is being realized from high level perspective. Service blueprint consits of actors (placed on vertical/Y axis) and steps to deliver service (placed on horizontal/X axis). It also includes visibility boundry, which allows to distinguish processes that are visible to user (above boundry) and those, that happen "behind the scenes" (below boundry).
H|*Replication events* streaming via audit log or change tracking/change data capture|Architecture/Distributed|H|Describe my approach to building local storages around autonomous systems/modules/microservices etc. with audit log scanning or - if available - scanning Change Tracking (SQL Server) or Change Data Capture (SQL Server or PostgreSQL) and publishing results as either strongly typed (if in homogenous environment) or a weakly typed (if in polyglot environment) replication events.
L|`AsyncLock` by Stephen Cleary based on Stephen Toub MSDN article|Languages/C#|M|Describe a `AsyncLock` implementation, that simplifies mutual exclusion in async-await scenario (using `SemaphoreSlim` and `IDisposable` with `using` pattern behind the scenes)
H|Mutex, lock, semaphore, monitor, barrier, spinlock, RCU locks|Engineering/Computer science|H|Describe synchronization implementations used nowadays, including mutex, lock, semaphore, monitor, memory barrier/fence, spinlock, read-copy-update and readers-writer locks. It wouldn't hurt to mention read-modify-write, fetch-and-add and test-and-set operations. Why `volatile` keyword is important should also be mentioned here.
H|Compare HTTP Codes vs 200 + `{ errors: [] }` approach to business error handling|Architecture/Distributed|M|Discuss on how and when HTTP code for each class of problem (4xx) in RESTful APIs would be a better choice than 200s + errors in payload. Perhaps there should be some middle ground (especially for business validations) introduced as *catch-all* mechanism? I can think of returning every business error as 422 (Unprocessable Entity) to indicate business error (monitoring or client-side libraries auto-detect 4xx and 5xx as errors) along with payload decribing what went wrong. I'm typically against using HTTP codes in general as results of business rules violation, because I treat them as *technical errors*. So either input was malformed (serialization issue?), server refused to authorise user or simply such endpoint does not exist. Or maybe server just exploded and all we know is that 500 - Internal Server Error. However, I can understand how this can be misleading, that API returns 200 - OK for errors, even if there is a reason in a payload. When exposed to the outside world, I would probably go with 422 route + `{ errors: []}` approach.
M|SQL Server security model|Engineering/Databases|L|Describe basics of SQL Server security model: login, mapping login to user on particular database, creating role on such database, assigning user to it and assigning permissions to certain objects to that role.
M|What is Istio?|Engineering/Distributed|M|Describe what is Istio and what problems it solves, ie. traffic management (routing, policies, load balancing, service discovery, staging and rolling releases etc.), observability (metrics, healthchecks, distributed tracing, access logs) and security (mostly certs and authorization, for ex. JWT access rules on service-level); also it would be useful to compare Envoy sidecar proxy Istio utilizes on service-level with kube-proxy in Kubernetes that works on node-level; briefly descrive data plane and control pane in Istio.
L|Kubernetes Ingress vs alternatives (Traefik, HAProxy, Istio Ingress)|Engineering/Distributed|H|Compare popular Kubernetes Ingress alternatives and write few bullet points when it's worth switching to one of them.
M|Domain (private) events vs integration (public) events|Architecture/General|L|Compare domain and integration events along with use cases;domain events - internal communication in-memory between aggregates in same bounded context/service boundry; integration events - out-of-process for external services, modules or even systems that are meant to do something with it in async manner
M|Working set vs private set|Engineering/Operating systems|M|Describe differences between private set (aka private bytes), ie. memory that process allocated (and is in use or paged out) and working set, ie. memory, that process is actively using (in main memory). There are also virtual bytes, which stand for total virtual adress space used by process. This metrics can be helpful in troubleshooting OutOfMemory exceptions caused by memory leaks (private bytes > working set), memory fragmentation (virtual bytes > private bytes) or simply when allocating more memory, than can fit in RAM + pagefile.
L|SQL Server - Instance vs VM stacking|Engineering/Databases|M|Describe differences (in cost, licensing, administrating and troubleshooting) for provisioning SQL Server databases with instance stacking (multiple instances on one VM) approach vs VM stacking (single instance per VM) approach. From my current notes it's worth analysing SQL Server and Windows Server licensing (per core vs per installation), resource sharing, maintainability (when multiple instances can have impact on each other on same VM) and so on.
M|Non-cryptographic hash functions (Murmur, FNV, SipHash)|Engineering/Computer science|M|Describe usage of non-cryptographic hash functions. Common functions are `Murmur`, `FNV` (Fowler-Noll-Vo) and `SipHash`; there is also `xxHash`, which claims to ran at close to RAM speed(!). Historically, most popular non-crypto hash function was `CRC32` (which is probably still in use, for example for data integrity check of zipped files). Since these hash functions are not (generally) tweaked to be used in security scenarios, they tend to be faster than cryptographic functions. They're usage nowadays is for data integrity checks (CRC and checksums), verifying uniqueness and for hash tables implementation (Murmur3 is used in memcached, Redis and Elasticsearch). Interesting comparison can be found o StackExchange: https://softwareengineering.stackexchange.com/questions/49550/which-hashing-algorithm-is-best-for-uniqueness-and-speed/145633#145633
M|Cache stampede|Engineering/Distributed|L|Cache stampede is a situation, when under heavy load multiple processes attempt to compute resource (for ex. web page) to cache upon cache miss. This can possibly lead to situation, when resource, that we attempted to cache will never be served to user due to timeout. This can also lead to general system instability to due to resource (CPU, network or IO) starvation. Possible mitigation can be locking or delegating resource-to-be-cached computation outside of the process (assuming it will be single place, it will know of all incoming requests to cache such resource).
M|Law of Demeter|Engineering/Design patterns|L|Law of Demeter (*"don't talk to strangers"*) is a rule, that states, that class C should talk (method call) only to it's own members, objects created by it, arguments passed to its methods and to global (static) objects. Therefore it forbids calls like `person.getAddress().getZipCode()`. Ofc in general getters are not an issue - actual business operations performed on objects other than "neighbours" are the issue here. They lead to coupling between objects, that is not so obvious when analysing given class.
H|.NET performance troubleshooting|Languages/C#|M|Describe commonly used tools to troubleshoot .NET applications performance. The ones that I've mostly used are *PerfView*, *DebugDiagTool*, *perfmon* with choses performance counters (depends on your usecase), *Visual Studio profiling session* (amount of calls and CPU time, managed heap structure etc.). There are also new tools in .NET CLI, especially useful on Linux environments, from which most of can be found on dotnet organisation on GitHub in diagnostics project (mainly `dotnet-counters`, `dotnet-dump` and `dotnet-gcdump`).
M|SQL Server - `WITH(NOLOCK)` vs `WITH(READPAST)`|Languages/SQL|L|Compare `NOLOCK` (== `READ UNCOMMITTED`) with `READPAST` hints in SQL Server. Both of these hints are used for speed optimisations, but they differ in a fact, that `NOLOCK` reads state of records, that are currently inserted/updated/deleted in a pending transactions (so *dirty read* occurs) wheras `READPAST` in a same situation will skip locked rows.
H|SQL Server - RCSI (Read Committed Snapshot Isolation)|Engineering/Databases|M|Describe how `RCSI` mode works in SQL Server. RCSI is used to achieve optimistic concurrency model in SQL Server by using version store in **tempdb**. All the current transactions running under (generally default) `READ COMMITTED` will now skip locking on read enitrely and will receive last committed version of read record from the moment of when transaction started. This does not changes how other isolation levels work (which is sometimes forgotten, by myself for example...). RCSI in most cases will increace concurrency of the database but will come at a cost of higher tempdb utilization (which should be closely monitored and perhaps some upgrades of this part of SQL Server setup should be planned). RCSI is often summarized as **writers do not block readers (and vice versa)**. It's worth remembering, that if there is a need to achieve non-blocking reads with writes, that do not update already stale data, a `SNAPSHOT` isolation level should be used!
M|Indexed view (materialized view in SQL Server)|Engineering/Databases|M|**Indexed view** is a SQL Server implementation of materialized view. To create indexed view you need to create simple `VIEW` with `WITH SCHEMABINDING` option and `UNIQUE CLUSTERED INDEX`. Indexed view (as any other materialized view implementations) stores data physically, just like tables do (in contrary to classic views). When querying against remember to use `WITH(NOEXPAND)` hint on SQL Server versions lower than Enterprise. It will force query optimizer to use actually use indexed view instead of using table behind it. When moving from backlog, compare indexed view with Oracle SQL and PostgreSQL implementations of `MATERIALIZED VIEW`.
L|`WITH SCHEMABINDING` options|Engineering/Databases|M|Why use `WITH SCHEMABINDING` option in SQL Server? I've found two reasons so far: first of all, if we want to avoid accidental breaking of views when source table is changed, schema binding helps, because it forces DBA that alters the table to carefully review changes schema-bound view too (because view will have to be recreated). Second considerations is for UDFs that compute scalar values - when we use schema binding on them, we let know query optimizer to analyze body of the function and to possibly skip *Halloween protection* measures (like Table Spool) when it's not really needed (boosting performance as a result).
M|`CancellationToken` in ASP.NET Core|Languages/C#|L|Automatic binding of `CancellationToken` as a parameter to controller action in ASP.NET Core allows some interesting techniques in typical OLTP systems. When a request is aborted (for ex. user aborts request before it finishes), cancellation is requested on CancellationToken bounded to called controller action. While this might be not so useful on Controller level, we could pass it further, for ex. to Query/CommandHandler `Handle()` parameter and introduce some sort of transaction rollback (or skipping `SaveChanges` on `DbContext` or skipping `Complete` on `TransactionScope`), when we decide to commit, but find out that user actually aborted request. This might seem more natural to some of the processes, where user expected action to be aborted but modifications were still applied to the system.
M|`async/await` behind the scenes|Languages/C#|M|Describe how `async/await` works behind the scenes, what is cost of it (generating compile-time state machine and running it with queuing and scheduling continuation execution). It would be worth mentioning how state machine behind the scenes looks like, what optimisations are made for already completed tasks and why state machine is `class` in Debug but `struct` in Release. 
H|GRASP (General Responsibility Assignment Software Patterns)|Engineering/Design patterns|M|**GRASP** is a set of nine fundamental principles in object design and responsibility assignment, first applied by Craig Larman in *Applying UML and Patterns*. <br> Those principles are (P - problem, S - solution):<br> `Information Expert` (P: Where do I assign responsibilities? S: To objects, that has data to fulfill them),<br> `Creator` (P: Who creates objects A? S: Object of type B, if B aggregates A, B records instances of A, B closely uses A or B has required information to instantiate A),<br> `Controller` (P: Who should be responsible for handling input? S: Controller dedicated to given use case or group of related uses cases),<br> `Indirection` (P: Where and how to assign responsibility, to avoid direct coupling, while keeping high reusability? S: Introduce indirection through additional object responsible for mediation),<br> `Low coupling` (P: How to decrease impact when changing one class to other classes? S: Decrease direct dependencies between objects.), <br> `High cohesion` (P: How to keep objects focused, maintainable and understandable? S: Object should keep only those responsibilities, that are highly related to each other.),<br> `Polymorphism` (P: How to make components pluggable or replacable with alternative behaviour? S: Identify points of possible instability ie. alternative behaviour and introduce interfaces, instead of concrete implementations there),<br> `Pure fabrication` (P: How do we achieve rules like indirection without polluting problem domain? S: We can introduce intermediate classes, that do not represent problem domain per se, for ex. domain services in Domain Driven Design).<br>
M|Cohesion (programming)|Engineering/Design patterns|M|Cohesion in programming is a measure of determining *to which degree elements inside a module belong together*. When we mean that class has high cohesion, it means that its methods have a lot in common and do not process lot of unrelated data. Types of cohesion are as follow (from lowest/worst cohesion to best/highest cohesion):<br> `Coincidental` - parts arbitrarily groupped together in a same source file but not really common (*Utility* classes etc.)<br> `Logical` - parts groupped by type of logic they represent, for ex. all the *Controllers* in MVC pattern in same folder (logic - Controller, but different use cases/functionalities)<br> `Temporal` - parts groupped by moment in time in execution of program, when they are processed (ie. functionalities run on startup groupped together fit in here)<br> `Procedural` - parts groupped by sequence of execution (before X do Y)<br> `Communicational/information` - parts groupped because they operate on same data<br> `Sequential` - parts groupped, because output from operation X is input to operation Y (so opposed to procedural cohesion, this one is more of a neccessity); an example would be - I guess - companion methods to template method pattern?<br> `Functional` - parts groupped because they all contribute to the same, single task
M|Bus factor|Soft skills/Project management|L|Bus factor is a risk management measurement, that helps to determine single points of failure when key technical expert is gone and information/capabilities that person had, have not been shared (that's what name comes from: *what if person X get hit by bus?*). Bus factor is calculated by person count that would need to disappear from process/project/etc. to for it to stall. For ex. if in IT project single person has knowledge how compoent X works and such knowledge has not been shared (and is currently hard to catch up), we can say that bus factor = 1. If we'd have three person with such competences, bus factor would equal 3.
M|Angular change-tracking|Languages/TypeScript|M|To implement binding and updating views, Angular implements change-tracking mechanics behind the scenes. For anything that happens in the browser, change tracking is implemented using Zone.js by *"monkey patching"* browser APIs, ie. `add/removeEventListener` functions, `Promise` APIs, `setTimeout` and `setInterval`, `XMLHttpRequest` aka XHR (AJAX calls) and more. For component state bound to views, Angular uses 3 change tracking strategies: `Default` (tracks fields of mutable object), `OnPush` (tracks only references, works best with immutable objects; works also with `async` pipe) and `disabled` (for manual firing change detection cycle). How detection mechanics (in `Default` strategy) actually know which fields to track? Angular analyzes syntax of component's template, find bindings and keep track of only those, which values changed (by comparing previous and current value of course).
M|Deferred vs immediate domain events raising|Architecture/General|L|There are two approaches/patterns to raise domain events in DDD. First one (I guess an older one) has been proposed by Udi Dahan and is a simple `EventRoot` static class that receives events, subscribes handlers and orchestrates forwarding events. I actually did this one as a Kata in devs-pl initiative here: https://github.com/devspl/Kata---Business-Rules/tree/master/m-wilczynski-csharp-payment_handlers There is also a second approach, proposed by (I think) Jimmy Boggard that we put our domain events into collection of events which is property of our domain object (so `IReadOnlyCollection<IDomainEvent> MyDomainObject.DomainEvents`). We then collect them on our handler/use-case level and publish them before or after commit (that's an architecture choice to be made here). Second approach makes it also much easier to unit test event flow, since we can use "*Chicago school*" (aka state school) of unit testing and assert state of an object (ie. collection of events) after we acted upon it (ie. did some business operation).
L|SQL CMD checking with `NOEXEC`|Languages/SQL|L|When writing scripts in T-SQL with SQL CMD mode on, you might want to check whether script has all the variables properly set and if SQL CMD mode is set properly. To prevent running some of the code or to validate it we can add `SET NOEXEC ON` to make SQL Server parse it, compile it but not execute it. To leave compile option out too we can use `PARSEONLY` instead.
H|Top 10 OWASP|Engineering/Security|M|**Top 10 OWASP** is a document from organization called Open Web Application Security Project, pointing most common security issues in applications. <br>These issues are:<br>- **Injection** (like SQL injection - placing untrusted data as a part of a query; vulnerable application would be the one, that does not sanitize user inputs before executing queries, leading to easy access to critical - from security POV - queries like `SELECT * FROM users`), <br>- **Broken auth** (weak session, password and auth management, ie. does not implement MFA, does not update max. length and complexity of passwords, does not check passwords against top X common passwords, does not delay failed logins to to make brute force attack impractical etc.) <br>- **Sensitive Data Exposure** (not encrypting data at transit and at rest aka used"plain text"; using old crypto algorithms; caching sensitive data; using weak hashes, not using salt etc.) <br>- **XML External Entities** (by default older XML processors would process external entities by evaluating them on processing; these could lead to Denial of Service vulnerabilities or probing private network resources for protocols like SOAP < 1.2 and SAML) <br>- **Broken Access Control** (not enforcing granular permissions or even not checking what user can and cannot do in a system, that may lead to performing unauthorized actions and/or accessing confidential data) <br>- **Security Configuration** (not setting up properly or at all access control for resources; not setting security-oriented HTTP headers, more: https://owasp.org/www-project-secure-headers/; not patching OSes, runtimes, frameworks and libraries in timely fashion) <br>- **Cross-site scripting (XSS)** (accepting untrusted/non-sanitized input from users, that is later on used to render other users' views, leading to execution of malicious scripts on client-side) <br>- **Insecure Deserialization** (not performing strict checks of incoming serialized data for non-primitive types, so binary serialization other than JSON etc.; OWASP mentions user session state deserialization, that has no integrity checks with means like signature check in JWT) <br>- **Using Components with Known Vulnerabilities** (common problem in large applications and/or organisations, where tracing every possible used library or framework and tracking all of the CVEs (Common Vulberabilities and Exposures) is hard, due to the complexity of the environment) <br>- **Insufficient logging and monitoring** (no matter how thoroughly you've checked your code, if you have no proper logging and monitoring to identify suspicious actions and no process to quickly escalate security exceptions - you'll fail)
M|Unit testing schools|Engineering/Testing|M|I've heared of **schools of unit testing (or TDD schools)** from some time, so I've decided to take a deeper look into it. It seems that there are two of them:<br> - **Chicago (also called Detroit) school of unit testing aka "State-based" aka "Classicist" aka "black-box"**: focuses on state-based testing, so if we perform operation `A` on `MyObject` (literally `MyObject.A()`), our assertions should be against the state of `MyObject` or at least about the result of the operation that we retrieved. <br> - **London school of unit testing aka "Mockists" aka "white-box"**: focuses on interactions between objects and assert the usage of dependencies and reaction to what they return. <br> I would say both schools have their points and should be used interchangeably. I'm more and more leaning towards state-based approach, but saying that I would not to double-check on how we handle (especially external) dependencies would be an understatement.
H|SQL Server - Statistics|Engineering/Databases|M|Every developer dealing with more than basic usage of T-SQL has gone to the point of at least acknowledging, that assumptions upon queried data that query optimizer makes are based on statistical data SQL engine collects called ***Statistics***. But what Statistics consist of? Statistics are BLOBs that hold data (of 1 or more columns) about distribution of values in a table (in a form of `histogram`), which allows query optimizer to estimate number of rows and to use best matching operator. Statistics also hold data about correlation of values in a table called `density`, which is based on number of distinct values in a column. Statistics can be viewed by either SSMS (under table element) or by T-SQL. <br> There are two strategies on maintaining good statistics and both should be employed: <br> - `AUTO_UPDATE_STATISTICS` (database option) - query optimizer determines whether statistics should be updated before executing query. Engine uses two factors to determine update (aka recompilation): `# of modifications since last recompilation` and `table cardinality`  (uniqueness of data values that table holds; high cardinality -> lot of distinct values). Auto update can be done synchronously (so query will wait for statistics update but will benefit for up-to-date statistics) or asynchronously with `AUTO_UPDATE_STATISTICS_ASYNC ON` (query will not wait for new statistics and will not benefit from them but will trigger the process of updating, so next queries will benefit) <br> - `Monitoring and manually updating statistics when needed` - when queries are performing badly and operators in query plan suggest unsufficient or lacking data for query optimizer to make right decisions, there might be a need for manual update with `UPDATE STATISTICS my_schema.my_table;` (or even more granular with certain table statistic) or going all-in with `sp_updatestats` to perform full database statistics update (which is risky, given how much resources it would take)
M|Performance counters for IIS-based web server|Enigneering/Operating systems|M|Watching your web server health is an obvious thing to do and on Windows Server based servers we can do so with performance counters. Note - this won't tell you much about ASP.NET Core apps performance, since they do not natively use Performance Counters at all. See `dotnet-counters` instead. <br>  PerfCounters that could prove to be useful are: <br> - `Memory - Available MBytes` - available RAM <br> - `Processor - % Processor Time` - CPU time, especially useful when comparing particular w3wp.exe process  utilization vs total system utilization <br> - `Virtual bytes` and `Private bytes` per w3wp.exe - can help pin pointing possible memory leaks and/or memory fragmentation issues <br> - `HTTP Service Request Queues - CurrentQueueSize` for HTTP.sys kernel side queue - requests waiting for IIS to pick them up <br> - `.NET CLR Exceptions` - for, well - exceptions thrown in total <br> - `Request in Application Queue` - ASP.NET (or rather CLR Thread pool) queue; sinces it's server-wide statistic, it can only tell if web server is reaching it's bottleneck (and reaching thread starvation I guess?) <br> - `Requests/Sec` and `Requests failed` per ASP.NET application can tell a little more about throughput and if there is no spike in failed requests (that could indicate problems with an app)
L|Implicit, explicit and auto-commit transactions|Engineering/Databases|L|Implicit, explicit and auto-commit modes for transactions are ways of RDBMS supporting (or not) wrapping statements in transactions. Default behaviour in SQL Server is auto-commit, which means that when we execute a statement, engine automatically starts transaction, wraps it around the statement and finally commits it (so we don't manage transaction at all). When we use implicit mode, engine only starts transaction and wraps our statement with it but does not commits it automatically. When using explicit mode, we are responsible for all the steps: `BEGIN TRAN` and either `COMMIT` or `ROLLBACK` of it.
M|Data compression - SQL Server|Engineering/Databases|M|Compression is something we come up with with when we want to save up some space. We can compress tables or indexes with `ALTER TABLE/INDEX dbo.index_or_table REBUILD WITH ( DATA_COMPRESSION = ROW/PAGE )`. What I found is more interesting - it seems that compression can actually positively affect database performance. In general compressing data saves disk space but uses more CPU, since data has to be compressed on writes and decompressed on reads. However, many blog articles suggest, that if I/O and/or memory pressure is an issue on your SQL Server instance, perhaps sacrificing a little of CPU can be worth trying out. It's especially recommended for read-heavy tables (citation and general evidence needed).
H|DDD patterns cheatsheet|Architecture/General|H|I've used bits of both strategic and tactical patterns from DDD for years now, but never have I took time to wrap them up in a quick, lookup cheatsheet. So, it would be worth writing a little (along with some personal thoughts) about: <br> - strategic patterns - "whole" `domain` discovering (users, requirements, usages, constraints) and its `ubiquitous language`, `bounded context`, `core domain` and `generic and supporting subdomains` that result from domain distillation along with all of the `context mapping` strategies, <br> - tactical patterns - `aggregates` as transcation/consistency boundries (along with basic design principles for building them, perhaps along with `entity`, `value object` and `aggregate root` as building blocks) and  `domain events` (I have both public vs private event differentiation and handling patterns topics already queued in backlog); <br> There's also a super-awesome starter kit from ddd-crew that I've forked that wraps it all really nicely: https://github.com/m-wilczynski/ddd-starter-modelling-process
M|EventStorming cheatsheet|Architecture/Business analysis|M|EventStorming as a "discovery technique" have proven to be extremely useful to my team in a last project. EventStorming session is performed on a timeline-like (flow of time goes right way) board with sticky notes as a collaboration tool. Heart of our EventStorming session would be identification of `domain events` (orange note) - concept from DDD (that doesn't neccessary need to result in pure DDD-solution on implementation stage). Domain events are also our starting point for first phase of our discovery session, where we would try to grasp high-level view on a business procesess in general (also called *Big Picture EventStorming*). After (hopefully) getting a big picture perspective and identifying subdomains of our problem space, we would move to the second phase - *Design Level EventStorming* (that sometimes gets splitted in an additional, pre-design phase of *Process Level EventStorming*). On this level we'd add `actors` (dark yellow note) that execute `commands` (blue note) upon `aggregate` (light yellow note) or generally on a `system` (pink note). Both command execution and its result are presented on a `view/GUI` (green note). There might by critical areas in a processes called `hotspots` (red, rotated 45deg note). Optionally, we can also add immediate (sync) enforcements - `rule/validation` (light blue) and async (as a reaction to domain event) `policies` (purple note).
L|Inlining hints for JIT Compiler|Languages/C#|M|Inlining is a process of replacing method call with method body if neccessary (ie. performance boost could be worth the duplication). In general, most obvious cases will be inlined anyway by compiler without any hints provided. This however might not be true for less obvious cases or for a hotspots, where compiler might not recognize the value, that inlining could provide. Or sometimes compiler does not have a strategy in place for such case. That's when `[MethodImpl(...)]` attribute comes to the rescue providing hints for compiler where inlining should take place (note: "should" != "will"). Mentioned attribute accepts flags of enum `MethodImplOptions` and those, that we should be interested in for this case are `NoInlining` (8) and `AggressiveInlining` (256). There are also few other interesting things in MethodImplOptions, with notable mention of `Synchronized`, that enforces lock on any caller of annotated method.
M|Data cardinality|Engineering/Databases|M|**Cardinality in SQL querying** (as a part of statistics) is a measure of how unique or distinct values are in a column. Low cardinality means lots of same values, while high cardinality means most of (or all) values are unique (think primary keys, GUID/UUID etc.). This metric is further used for query optimizer to make decisions on what query plan it should use, as it partially affects things like query selectivity (needs additonal note). <br> But (!) there is also a term of **cardinality in monitoring**. Consider monitoring data as a set of `metric_name` - `dimensions` - `metric_value`, we would say that high cardinality of monitoring data means that any of the metric dimensions has lots of distinct values (so actually in definition pretty close to SQL terms). Let's give an example: if we have metric of cpu.utilization: `{metric_name: cpu.utilization} {dimensions:[{'server': srv01}]} {metric_value: 0.7}`, then if we have 200 servers that we monitor CPU on, each one of them would have different value of `dimensions.server` and would mean high cardinality. It's important, since if we want to query data by dimension, it needs a good indexing strategy. If we have 200 distinct servers, that's not that bad (but still 200 servers x heartbeat per 5s everyday). But if we add another dimension (for example `service_name`), we now have cartesian product of both dimensions, exploding number of series by an order of magnitude.  <br><br> Wrapping up - for SQL querying high cardinality is good - if column used for query has high cardinality, then using it in WHERE clause gives us high selectivity.<br> For monitoring high cardinality is bad, because it leads to higher number of time series, affecting time series database performance due to higher resources requirements to serve user queries.
L|String interning|Languages/C#|M|String interning is a technique used by .NET runtime to store and reuse string literals (so "hardcoded" strings) instead of reallocating them over and over again. So if we have somewhere in code literal like `var myPrefix = "PREFIX_"` and in some other part of the code we have the same literal declared, we would not get newly allocated string with same value, but we'd receive the same tring with the same address in a memory (so no copying and allocating involved). <br>Behind the scenes, interned strings are stored as any other string - on SOH or LOH depending on their size. Their adresses are stored on special structure called **String Intern Pool** (placed on LOH) and further "registered" in a *String Literal Map* hash table (on .NET Framework unmanaged private heap). This hash table is looked up on any attempt to intern string and if such string is already interned, existing interned string address will be returned instead.
L|`ArraySegment<T>` vs `Span<T>`|Languages/C#|L|`ArraySegment<T>` struct has been introduced very long ago - in .NET 2.0, but actually was quite rarely used (in my opinion). Its use is to provide a "sliced, mutable view" over a part of an array without actually allocating new array. It became much more useful since .NET 4.5, when it started to implement `IEnumerable<T>`, `ICollection<T>`, `IList<T>` and `IReadOnlyCollection<T>` (all the LINQ goodies and so on). `Span<T>` ref struct is a much broader (or rather general) implementation of "mutable view" on part (slice?) of a data (actually - a contigous region of memory) that has been allocated on either managed or unmanaged heap or on stack with `stackalloc`. Span is always allocated on stack - there is no way to allocate in on heap (it's not allowed to be field of a reference type that would make classic struct to be placed on heap, boxing of it is not allowed etc.). For a "read-only view" over a contigous region of memory use `ReadOnlySpan<T>`.
M|Redis transactions|Engineering/Databases|M|Although **Redis** is usually used as a plain-old cache solution, it doesn't mean it has no more advanced features. One of them are transactions, which are implemented in a little less "complicated" way than those in SQL engines. Redis offers `MULTI` command as a way to start a transaction and `EXEC` as an execution of all of the queued commands after `MULTI` command. `EXEC` executes all of the commands sequentially and guarantees, that no other client will perform any operation in-between. Note: all of the commands are executed the moment `EXEC` is fired! <br> Also, there is a way to abort transaction with a `DISCARD` operation (which essentially dequeues previously queued commands).<br> What's more interesting is that Redis also supports optimistic concurrency via `WATCH` command. `WATCH` is essentially a "check-and-set" operation that tracks given key and commits transaction via `EXEC` only if watched key did not change in a meantime.
M|TDD vs BDD|Engineering/Testing|M|Since I've made multiple (sometimes more, sometimes less successful) attempts to incorporate TDD to my everyday workflow, I've found myself walking the (in my opinion) same worthless path of testing every damn thing, neglecting the real value tests should offer to business stakeholders. The quality of tests have of course increased with TDD a lot, since implementation does not have to take testing "into considerations" - it's builtin. However, I kept on missing the business value that those tests could present so not so long ago we've introduced annotations to our Selenium tests with `Fody`, to record every step that happened throughout the test execution. I liked it a lot, since we had "test scenarios describer" built-in, but that was still a "post-factum" addition to the test, instead of being a starting point for it. That's where BDD came into my mind, as I've read about it a long time ago and I guess now pieces start to come together. Provided we have specification in place (for ex. as a formalization of discovery stage), we could easily provide lots of human-readable `Given-When-Then` scenarios (in Gherkin or anything that fits), that could be a "common-ground" for developers, QAs and business (like Product Owners) to talk about during the development process.<br> So, to put TDD and BDD in comparison, I find TDD as a development technique, wheras BDD is an evolution of TDD that pushes it on "higher-level", making it a team/development process practice.
M|Edge vs fog vs cloud computing|Architecture/Distributed|L|When diving deeper in a world of cloud computing I've figured out another "types of computing" that gain traction recently. One of them is (I guess more popular) `edge computing` and another one is `fog computing`. While cloud computing offers an almost unlimited scaling, storage etc. (and lots of other cool things like infra-as-a-code, managed services like k8s, identity providers etc.), it's often inefficient to send all of our data to cloud at once. That's where edge computing cames into action, offloading part of the computing and preprocessing to the edge servers (servers near our collected data, so for ex. near IoT sensors). To further improve latency, reduce storage usage in the cloud (think costs) and perhaps filter out some of the not needed data, we can introduce concept of fog computing - to have smaller data centers between edge servers and the cloud, that would be capable of receiving data from edge layer and forward them to cloud only if needed and for ex. only on give part of day (to better utilize resources in the cloud etc.). Since fog layer is meant to be closer to edge layer, latency would be much lower than than connecting directly to the cloud, so sending data from things like IoT sensors can be done immediately, instead of batching etc.
M|Storing JSON in SQL Server|Languages/SQL|M|There are two ways for storing JSON in SQL Server. If you plan to use JSON querying (and other features) available on 2016 version and onwards (or on Azure SQL), you should use `NVARCHAR(MAX)`. However, if you only intend to store JSON data in database but not really intend to query for values inside it, you can store it in `VARBINARY(MAX)` field. What's even more interesting, is that we can use `COMPRESS(<string>)` function on inserting JSON value to the VARBINARY column (to better utilize our storage) and create another, computed column `json_as_text AS CAST(DECOMPRESS(json_as_binary) AS NVARCHAR(MAX))` that will decompress and convert JSON to text representation, effectively offloading (and simplifying) consumption on application side (at a cost of CPU on database side).
H|.NET Garbage Collector in-depth|Languages/C#|H|Probably every .NET developer had to learn some basics  how garbage collection works in .NET (or in similar languages) and things like generations, Mark and Sweep, SOH and LOH. But how actually does it work underneath? When does it occur? What are the consequences? Here - obviously - comes Konrad Kokosa to the rescue with his Pro .NET Memory Management book.<br> So we know, that garbage collection happens on managed heap (one for Workstation mode, one per logical core for Server mode), where dynamic allocations occurs (concept of **Mutator, Allocator and Collector** trio would be worth mentioning here). Physically, heap is organized into **"segments"**, that either belong to SOH or LOH.<br> When new object is allocated, decision on where to place it is based on its size: **smaller than 85000 bytes - SOH, larger - LOH**. We call it *"size partitioning"* of memory. While most of the logic/code of those two is shared, the fact that LOH holds large objects makes it impractical to run "Compact" phase of GC on it (Mark and Sweep is actually Mark-Plan-Compact-Sweep in .NET).<br> SOH is further partitioned by allocated *objects lifetime ("lifetime partitioning")* into **Gen0 ("young"), Gen1 ("temporary") and Gen2 ("old") generations**. Lifetime is "measured" by *amounts of GC runs object survived* (so survived 0 times -> Gen0, survived once -> Gen1 etc.). Interesting fact about *generations* is that from memory management perspective, they *are just memory boundaries*. So promotion to higher generation is a movement of a generation boundary. <br> When actual GC occurs it first starts with deciding which generation to collect. Even though GC can be called to collect particular generation (let's say Gen0), it can decide before running, that it will "condemn" higher generation (for ex. Gen1). *Along with condemned generation all of the younger ones are collected too* (so if Gen1 is picked, Gen0 is also collected). **If Gen2 is condemned, we are calling it "full-GC"** - since LOH will also be accounted as Gen2, it simply ends up as all of the generations get collected. <br>When running, GC is divided into: <br>- **Mark** Phase (that starts from so called "roots") that looks for all of the reachable objects from root as a starting address; if object is not reachable, it is considered "dead", <br>- **Plan** Phase, that decides what to do with dead objects - should it Compact or just Sweep; Compact is more expensive but lead to (almost) no fragmentation, whereas Sweep can leave us with fragmented memory, but that could be fine, if we are allocating there pretty frequently (so fragments of heap will stay there waiting for new objects as reserved memory) <br>- **Compact** Phase - this is where objects are compacted/squashed together, generation boundaries are moved and references addresses have to be fixed, pinned objects checked etc.<br>- **Sweep** Phase - space taken by now "dead" object is turned into "free space" - ready to be allocated again<br>Since some of the parts of garbage collection process cannot operate properly on memory if something else is mutating it, suspension mechanics has been introduced, called **"EE Suspension"** (Execution Engine suspension); this probably needs another note, since Konrad explains a little deeper how code emitted by JIT lets GC know which *parts of the code are partially and which ones are fully interuptable*.<br> But what actually triggers garbage collection?<br> There are many reasons (written down in Konrad's book), so I will note only those obvious to me:<br>- low memory alert from system <br>- failure on allocation by Allocator <br>- manual `GC.Collect` <br>- allocation budget reached<br> Also - I've mentioned Worktstation and Server GC Modes - they should be described in more depth as an additional note, since they do matter on how GC will works. Most important from web developer perspective should be older **Server Non-Concurrent** and newer (net450+) **Background Server**. I'd rather ommit low latency modes in GC, since I'm not really into desktop .NET programming. 
M|Troubleshooting running pod (or container)|Engineering/Distributed|L|Since I have no prior k8s production experience, it would be worth noting down possible ways to debug and troubleshoot pods running on it.<br>Basic stuff that comes to my mind is: <br>- browse the state with `kubectl get po {pod-name}` and `kubectl describe po {pod-name}` <br>- browse the logs with `kubectl logs {pod-name} {container-name}` (optionally with `--previous` if you're after the crash) <br>- running some shell scripts inside it with  `kubectl exec {pod-name} {container-name} -- {my-command}` <br>- if node is simply not reachable from outside, perhaphs something is wrong with our service; try `kubectl describe ep {service-name}` <br>- we can use ephemeral containers (still in alpha) to start another pod on same node as the pod we want to debug and try interacting with it
 H|Specification pattern aiding Repository pattern|Engineering/Design patterns|M|In Domain Driven Design, there are two common patterns used to distill and encapsulate domain rules into strongly-typed constructs, namely: **Specification** pattern and **Policy** pattern (which is closely related to *Strategy pattern*). Former one describes features/facts with a boolean/predicate logic, whereas the latter describes actions. Great thing about them is that we can easily reason about features or operations in a fully typed manner, so instead of boolean checks on many fields/props we can encapsulate them into business-named predicate.<br> Recently I've heared Steve Smith (aka "Ardalis") talking about usage of Specification pattern as a great augmenter for Repository pattern. I've dug a little into it and found blogpost by (as always though-provoking) Vladimir Khorikov, who in a very simple manner shows difference in usage between classic Generic Repository with `Expression<TEntity, bool>` as argument vs Specification as an argument. The most interesting thing is how simple specification named with a domain-specific language can be much more expressive than Expression with very little work needed to be done. If we assume, that Specification<TEntity> has some input for a specific predicate to check upon, the only thing we need, to make it work with our persistence layer (think EF Core or NHibernate) is to prepare some common method like `ToExpression()` that will convert Specification into Expression and allow ORMs to convert it to SQL language.
 L|SBOM (Software Bill of Materials)|Architecture/General|M|Software Bill of Materials is an idea taken from traditional manufactoring, where we can easily retrieve from what parts, that is libraries, frameworks etc. and in what versions software we built or bought is made of. That makes it possible to make sure on what technology, vendors and subcomponents we actually depend on and easily find security vulnerabilities (for ex. analyze CVE notifications for such library - that's something that already GitHub or NuGet does) or licensing problems (we are using libraries, that do not allow commercial use etc.). If we use some sort of centralized solution for gathering SBOM per application/component we have in our portfolio, we can easily analyze risk we are facing (or are going to face in a nearby future). Example of SBOM solution to "pipe-in" into CI/CD pipeline is DependencyTrack: https://dependencytrack.org/
 M|Git Internals - Objects|Engineering/Version control|M|As a version control system, Git internally works in a way similiar to key-value store. It consists of three types of objects: `blob`, `tree` and `commits` stored in so called `object database` backed by *.git/object* directory in a repository. Every object is stored using checksum - SHA-1 hash and organized with first two chars being folder name and rest being filename, so for ex. **b1***f8b86cda1d97694d2d7db3a23f6f759c85848e* would be placed in *.git/objects/**b1*** folder with filename *f8b86cda1d97694d2d7db3a23f6f759c85848e*. To browse any object in repository, we can use `git cat-file -p <object-hash>` to prettyprint this object (or use `-t` flag instead to determine its type). <br> As for the object types, `blob` stands for a file (without a name!), `tree` stands for a directory (which can hold blobs and other trees along with their names) and `commit` stands for - well - commit, that every git user knows. For a blob, `git cat-file` will return its content, for a tree it will print out its contents (so blobs and/or trees) along with names and for commit it will print out commit metadata (so tree hash, parent commit hash, committer etc.). <br>
 M|Git Internals - References|Engineering/Version control|M|Knowing  what objects Git consists of can make us wonder how does it actually know which commit stands for which branch and how to easily figure out which one is the newest. That's where references come to play. <br>Conceptually reference is simply a "symlink" to a given hash residing in `.git/refs/` directory. There are three main types of refs: `heads`, `tags` and `remotes`. Heads are the heart of a git branching - they store the hash of a last commit on each of a branch. Tags are utility references, that do not move (as opposed to HEAD) and can be used to tag commit, from which deployment or artifact creation has been performed. Remotes represent remote Git repositories to which we push our code. On client (developer) side they are stored with their address along with last known state of each branch (something we would update with `git pull`). In general you'd only use `origin` remote (though tools like *Gerrit* use another one for `git review`).
 L|SQL Server - Query store|Languages/SQL|M|There are many tools that can be used to track query performance in SQL Server - Profiler, Extended Events or Dynamic Management Views. Another tool has been introduced in SQL Server 2016 called **query store**. Great thing about it is that once configured, it will collect data (and persist it) from particular database (since it's "per database" feature) about CPU and memory usage, query performance, query executions per timeframe and query regressions. Its architecture allows it to capture queries "in flight" and to flush them do disk on configurable intervals (so no noticable impact on query performance).
 L|NUMA architecture|Engineering/Infrastructure|M|NUMA (Non-Uniform Memory Access) is an architecture, that allows multi-CPU systems to be aware of their local (on same node as CPU) and remote (on another nodes) memory through common bus. NUMA's greatest advantage is that it allows building much more cost-effective systems (so multiple cheaper CPUs and memory on single board) that are still visible to OS (and running applications) as single, continous memory (with single address space) ready to be used. Given that server OS (and runtime) supports NUMA, applications can use fast, local to CPU memory and (if local memory is full), still use remote memory on another CPU node.
 M|MVC pattern is dead, long live API endpoints|Architecture/Architectural patterns|L|If we take cohesion as one of quality indicators of OOP (and other paradigms of) code, we will quickly find out (if we're working as .NET, Java, Ruby, PHP etc. developer), that controllers in MVC pattern are particullary low on cohesion side of things. The same as "big repository" or "big business logic layer" that we got (at least partly) rid of with CQRS - they tend to group code mostly by "feature type" or technical similarity or even common address prefix in HTTP routing. What we end up with is code, that is not groupped by data it operates on or feature that it delivers, but rather on a very abstract idea of controller and/or common routing key, that they all fit on, that we could further abstract as... category of features groupped by common entry point?<br> That's an issue that API endpoints try to fix - to make particular route mapped directly to particular use case (think XYZHandler in CQRS), making all of the anachronic MVC bloated boilerplate go away. In ASP.NET Core, if you can't wait until ver. 6 (or you have not yet adopted it), you can still try approach introduced by Steve 'ardalis' Smith, that wrangles Controller base class from the framework to produce focused, use-case oriented, one action controllers.
 M|Source generators in .NET|Languages/C#|M|Source generators are meta-programming tool, that allows .NET developers to emit source code as a part of compilation pipeline. When compared to good old T4 it does feel (and work) much closer to your code, since it works as a library (akin to Roslyn Analyzers) and has access to already compiled source code. Although (same as T4) source generation cannot modify code (only can add it), it still has a lot of usages like compile-time dependency injection, compile-time automapping, contract-based proxy generation (gRPC but also some strongly typed REST endpoints), `INotifyPropertyChanged` auto-generation etc. Since source generators are compilation pipeline feature, they will work in any of the environments supporting .NET 5 SDK and onwards. Produced library is (to my understanding) targeting `netstandard2.0`.
L|Clearing query/SP cache - SQL Server|Languages/SQL|L|Sometimes we find out that already cached execution plan is either non-optimal (rather rare case since plans get evicted by engine anyway) or perhaps we're trying to optimize some query/SP and we made optimizations elsewhere (with no script changes), so we want the plan to be recompiled. We have few options, with most popular being: <br>- `DBCC FREEPROCCACHE <plan_handle>` with plan_handle taken from `sys.dm_exec_cached_plans` (beware of executing this DBCC without param - it will evict all of the plans!) <br>- `EXEC sp_recompile <related_table_or_proc_name>` <br>- `WITH RECOMPILE` in SP definition (not so flexible) and statement level `OPTION (RECOMPILE)` which can be used "ad-hoc" <br> Sometimes it might be worth dropping in-memory buffers to mimick fetching initial data from disk with `DBCC DROPCLEANBUFFERS`.
M|Hard and soft page fault|Engineering/Operating systems|M|Page is a fixed-length block used in virtual memory in operating systems. One of most common problem dealing with pages and virtual memory in general is **page fault**. Page fault occurs when requested address on a page is not in a memory used by current process (ie. `working set`). <br>We can differentiate two types of page faults: <br>- ***hard page fault*** occurs, when requested address resides in page that has been paged out to page file (or elsewhere to disk); access to such page is expensive, since it has to be fetched from disk to main memory to be used <br>- ***soft page fault*** occurs when requested address is in main memory, but it might be in a working set of a different process or a page waits to be paged out (that seems to be an optimization on OS level, to wait and see whether safe page fault occurs and page should be "rewired back" to page table or should it be really paged out since it won't be used soon) <br> Knowing general cost of fetching from main memory and from disk, we can expect soft page faults to be a minor issue whereas large rate of hard page faults can indicate bigger problems. It should also be noted, that (at least for Windows and partially for Linux) page faults are normal thing, since they are a way to optimize memory management using virtual memory by exceeding  actual physical memory size.
L|RabbitMQ - stopping publishers while maintaining consumers with "drain" strategy|Engineering/Middlewares|M|There are times, when you need to patch (or teardown) your RabbitMQ hosting environment. That might be OS change, breaking update or any other viable reason. Even if you're publishing messages as persistent (`delivery_mode=2`), it won't help you with mentioned scenarios, since you'd be tearing down Mnesia database too.<br> Approach that I came up with (and found that people like `MassTransit` author does too) is to stop publishers from publishing to message broker while maintaining consumer connections until queues are empty. This scenario is called ***"drain"***. Since your RabbitMQ cluster can serve many different publishers, there might not be an easy way to determine which publishing applications you (as administrator) should stop. However, there is actually an easy way to implement this on message broker level. If you know that you need to stop publishers from publishing, you can take advantage of memory or disk alarms, that are used for "backpressure" when message broker cannot keep up with publishing rate.<br> If we **set memory alarm to 0MB**, it would mean that - so called - `high water level` starts at 0MB allocated RAM on broker. That literally means, that broker always reaches memory alarm and it should stop accepting messages. This gives time for consumers to "catch up" and consume all of the messages, which allows us to proceed with our maintenance afterwards.
M|Model schema - in app (NoSQL) or in db (SQL)|Engineering/Databases|M|I've been "thought-provoked" by one of the podcast episodes (I guess it was one of Hanselminutes episodes about MongoDb) on why do we choose relational vs non-relational databases for our projects. One of the things (besides obvious ACID thing) I've been thinking about was model schema and why strictness of a relational DBs is a plus. The more I thought about it, the more I've started to realize, that while aditional type-check in RDBMS is a plus, the real type check and any other business validations is still performed on application level. It's driven by many reasons, but one of the most prominent recently is a rise of DDD and "clean architecture" (at least in .NET and Java space) that encourages us to implement businenss logic at the heart of our application. This further leads to "domain/core layer" being totally persistence agnostic, which forces us to perform schema validation in application anyway. The question now is - does model schema in RDBMS is still a plus, when we tend to model application regardless of how it looks like? 
L|Theory of Constraints|Soft skills/Project management|M|**Theory of Constrains** is a management philosophy coined by E.M. Goldratt in book "The Goal" (1984), that assumes that companies are always bottlenecked by certain constraints they have. We could paraphrase it as "company is as strong as its weakest link". If we'd try to adapt it to IT Project Management space, we would find many similiarities with already widely-known "Conway's Law" and PERT critical path.<br> An actual example on how Theory of Constraints can be used (and actually somehow was used in recent decade) is how we're trying to achieve flexibility in IT Operations space. Originally, development part of IT became much more flexible and adaptive due to introduction of Agile methodologies, but Ops got left behind. That's why DevOps philosophy has emerged to solve this "weakest link" that constrained organizations to excel in how they develop, release and maintain their software.
M|Tipping point - seek + lookup vs scan|Languages/SQL|L|In SQL Server ***tipping point*** stands for a amount of data, that would be "touched" by query that would make query optimizer decide to use clustered index scan instead of non-clustered index seek + key lookup.<br> Why would it occur? In general if our non-clustered index is non-covering, we would suffer from key lookups. If SQL Server predicts that query that uses such index with seek and key lookup ends up making more logical reads than scan would do, it fallbacks to clustered index scan. Prediction is made using - of course - statistics.<br> How can we prevent such situation though? If we can - reduce key lookups by widening the index coverage (for ex. with `INCLUDE`). We could also narrow the amount of rows returned (with things like pagination) or narrow the amount of columns returned, that would possibly lower the cost of lookups (but why not include them in an non-clustered index then?).
L|Skeleton placeholders|Engineering/UX|M|Skeleton placeholders are a way of presenting loading content of a part or a whole page to user. Skeleton placeholders present elements of UI for which data is being fetched with circles and rectangles. When data is fetched, those parts are replaced with actual data. They way that it differs from classic splash screens (used more commonly in mobile apps) or empty page with spinner is that it tricks user into thinking, that app actually loads faster, because the actual parts are there - they're just not filled with actual data yet. This could be especially beneficial for either first page in an app or any other pages where are focusing on keeping user on page (like products list, social media etc.).
M|VUCA in business|Soft skills/Project management|M|VUCA - Vulnerability, Uncertainty, Complexity, Ambiguity - is a term coined by US Army after the Cold War. It has been adapted in 2000's in business aspect as a way of describing environment in which enterprises live nowadays. Embracing difficulties of today can help us understand, that predictable, hard and exact goals are not really implementable in current world and we must strive for a different approach. Alistair Cockburn (one of Agile Manifesto creators) in his "Heart of Agile" movement tries to address complexity of Agile with simple rules, that offer guidance or "compass" - direction in which we should go with no hard goals. The reason he states in a such way is "because world is VUCA" and thus, we should be prepared to adapt and react quickly to how our environment changes.
L|Avalanche effect|Engineering/Security|M|`Avalanche effect` is a desirable property of cryptographic algorithm, when a small change in an input results in large changes in an output. Let's take a typical cryptographic hash function like SHA-2 as an example. If we decide to use it for storing hashed passwords in database, adding single additional digit at the end of passwords will result in dramatically different hash as a result. This makes results of such algorithm less predictable for potential break attempts.
L|Runbooks|Engineering/Non-technical practices|M|**Runbooks** are - in simple terms - a *how to* articles, that are mainly used by IT support and IT administrators. They are meant to be a simple to use, summarized articles for common issues and solutions along with routine tasks, that are described step by step. A runbook might be solution a common issue, that is currently dealt with with manually - for example step-by-step on how to run a manual virus scan from management console on user's machine. Or it might be a how to prepare a new machine for new employee, starting with some sort of "golden image" for his/her role and installing additional, specific software required for such person's job. Or it might be a handful of bullet points, that should be check one-by-one, when dealing with disasters like network outage.<br> While runbooks used to be a manually updated, trivia articles, they are now used by software solutions like PagerDuty or Atlassian Jira to prepare semi-automated tasks for their respective platforms.<br> What I found really useful in *runbooks*, is that they're an ideal starting point for automatic manual task, since most of runbook templates have prerequistes, steps to perform and things to watch for - everything that would be crucial, to prepare production-ready automation script.
L|`import`+`export` in ES6 vs `using` in C#|Languages/JavaScript|M|I've been recently thought provoked to compare how `import` and `export` in JavaScript since ES6 (and in TypeScript and some SPA frameworks) work compared to C# `using`. Classical C# `using` uses type imported from the same assembly or imported from another, dynamically linked assembly. This assembly must be referenced as a whole for the project to compile. As for ES6, you could `import` particular thing from another library and leave the rest of the library behind. The way it works is perhaps why **tree-shaking** is a standard procedure in transpilation process in JavaScript pipeline, whereas in C# there is only some experimental feature for self-contained deployment model called **trimming**, which is (I guess) still in preview as for .NET 5.
H|Dealing with change in Event Sourcing|Architecture/General|M|Event sourcing is an approach of modeling your application state using event streams (literally collection of events per aggregate or business entity etc.) that get replayed chronologically to retrieve current state, which we need to perform some sort of business operation. Since there is no notion of "current state" in persistence of such architecture, we need to keep all of the events, that lead to current state of aggregate. This however gets tricky pretty quickly, because schema of events would change over time and simple deserialization of event state would not be so trivial as `JsonConvert.Deserialize<MyEvent>` (or any other function in library you're using).<br> My current approach is to:<br>- decide on a `versioning strategy`, that would give me **event discriminator** (so event type name not exactly mapped one-to-one to my actual type in code) **with version**; if that's classical event sourcing solution, one would probably be able to duplicate multiple types with some sort of namespaces strategy like `V1.MyEvent`, `V2.MyEvent` etc. (since you're in charge of the event schema changes)<br> if that's a collaborative event sourcing (so you're building up state of a process based on events coming from outside of your system), that would not be possible, since changes to the event schema are out of your control; if event schema is somehow versioned (think SemVer + type is shipped as a package), I would just pick major version, since only majors should introduce breaking changes;<br> regardless of a chosen strategy, we could further treat event discriminator as a stringified name+version like: `MyEvent_V1` for storage purposes, <br> - use `weak schema` (like JSON), since its a must for storing multiple versions of a same type; so no fully qualified names of a type etc. <br> - use `upcasting` to deal with projecting any historical event schema to currently acceptable version (the one, that current business process understands); any defaulting, compensation, transformation etc. gets done here, based on a received type;<br> so `MyEventUpcaster` could analyze event based on discriminator and for current version `MyEvent_V2` simply deserialize payload and for `MyEvent_V1` compensate on missing values and remap some properties which names changed etc.<br> upcasters can easily save your ass if events come from outside and you are not yet ready for schema change that occured (but you got the event anyway, so you need to just quickly react with new upcaster for this version - no state is lost, just eventual consistency takes... longer than it should) <br> - if events come from outside, I would `ALWAYS model my private version` of them, so that I have single point of truth to what I'm reacting to in my business process<br> - use `snapshots`, to lower the burden of fast growing database, lower replay time and to minimize possible versions to support on upcast; <br> - `extensively test all of the supported events`, so that I'm sure, that all of the versions of events that I'm aware of are covered; that would not save me from changes outside of my scope, but it will cover most of the cases
M|Linux filesystem hierarchy standard (FSH)|Engineering/Operating systems|M|As a longtime Windows user and a rather newbie Linux user, I've have multiple gaps in understanding how it works and how it's organized. One of the things I've learned recently is how directory structure is organized in Linux.<br> Coming from root (`/`), we have: <br>1. essential binaries available before mounting `/usr`, that is: <br>- `/bin` where all of the all-user essential binaries reside like `cat`, `ls`, `cp`, `cd`, `ps`, `grep` etc. <br>- `/sbin` - same as `bin` but contains binaries available only to superusers, like `chroot`, `iptables`, `lvm`, `fdisk`, `ifconfig` etc. <br>2. `/boot` where bootloader (like *grub*) files reside <br>3. `/dev` where all connected devices files (including terminals: `/dev/tty`) reside <br>4. `/etc/` where system-wide configurations resides, like `/etc/hosts` or `/etc/resolv.conf` <br>5. `/home` where home directories of each user reside (equivalent of `C:\Users\` in Windows) <br>6. `/lib` where libraries essential for binaries in `/bin` and `/sbin` reside <br>7. `/mnt` - where temporarily mounted filesystems reside (like external/non system disks?) <br>8. `/opt` where optional software packages reside <br>9. `/srv` fron where server-role specific data is server like `www`, `rsync` or `ftp` <br>9. `/tmp` where temporary files reside <br>10. `/usr` where general (non-essential) system-wide binaries reside: <br>- `/usr/bin` - same as `/bin` but non-essential <br>- `/usr/sbin` - same as `/sbin` but non-essential <br>- `/usr/lib` as `/lib` but non-essential <br>- `/usr/local/*` where binaries, libs and scripts go, that are not managed by system packages (so your own scripts go here) <br>11. `/proc` where (I think?) virtual file system for each process resides <br>12. `/var` where dynamic program data resides (think of a files "produces" by binaries running on a system) <br> 13. `/sys` where system information is served as virtual file system
M|Avoiding LOH issues with `RecyclableMemoryStream`|Languages/C#|M|When using `MemoryStream` frequently in recent projects (for copying, exposing `byte[]` as stream etc.) I've - obviously - been dealing with LOH-related problems. At first I've been thinking on pooling arrays with `ArrayPool` and somehow chunking and chaining arrays, but later I've figured out that Bing team has open-source production-ready solution for this case on [GitHub](https://github.com/microsoft/Microsoft.IO.RecyclableMemoryStream).<br> It's called `RecyclableMemoryStream` and it's meant to be drop-in replacement for `MemoryStream`, sharing similiar API, incurring minimal Gen2/LOH allocations and avoiding memory fragmentation.<br> Behind the scenes it uses chained buffers that it borrows from small pool and large pool, that it are managed by `RecyclableMemoryStreamManager` (which is also used for creating such streams). Borrowed buffers are returned to pool on `.Dispose()`. If such stream is not properly disposed, there is still finalizer introduced by authors, that will eventually be called when there is no reference to such stream left.
H|SIMD operations|Engineering/Computer science|H|**SIMD** operations are set of instructions introduced to CPUs to allow processing multiple data with single operation (hence the name: Single Instruction Multiple Data). Because of this its sometimes called data-level parallelism.<br> SIMD operation works on a vector types as opposed to classic CPU operations working on a scalar data types. To allow "acceleration" via SIMD, there is a need to transform scalar to vector type which is sometimes called as `vectorization`. Vector types that fit in for a single SIMD instruction are sometimes called *SIMD units*.<br> SIMD operations were commonly used in graphic and audio processing, where many operations on matrices are performed. Nowadays it's also useful in machine learning, where many matrix operations are performed. <br> Common SIMD hardware acceleration instruction sets on x86-64 architecture are SSE (SSE, SSE2, SSE3 and SSE4) and more modern AVX (and AVX2). To support processing vector types new registers were introduced:<br> - for SSE: 8x 128-bit registers XMM0-XMM7 for 32-bit and 16x 128-bit registers XMM0-XMM15 for x64,<br> - for AVX: 16x 256-bit registers YMM0-YMM15<br> In .NET, there are specialized types introduced for SIMD operations in `System.Numerics` namespace and `Vector.IsHardwareAccelerated` property to check, if SIMD is supported. Even if SIMD is not supported, vector types in .NET will still work but without data-parallelilsm advantage.
M|Automated API tests with Postman CLI - Newman|Engineering/Testing|M|Postman is an known tool for testing HTTP endpoints. Although I'm more of a fan of Insomnia, I've definetely used Postman for many years in web development. Postman allows saving your HTTP requests you're using in so called "collections". Interesting part of introduction of Postman's CLI - ***Newman*** is that you can now use such requests for automation of so called `API tests`.<br> There are of course tools like Selenium (to test interaction with GUI) or JMeter/Taurus (to load test your API), but both of them are too slow to be part of an actual CI/CD pipeline, except being sort of quality gate. Newman however could be used for both post-deployment check and as crucial endpoints probing tool for your monitoring software.<br> You'd of course want to have some sort of healthchecks in place anyway, but that's a different topic than API tests per se.<br> API tests have of course some overlapping areas with both integration tests and contract tests, so one'd need to carefully think before duplicate ones tests.
|H|VRRP (Virtual Router Redundancy Protocol)|Engineering/Computer networks|M|`VRRP` is a protocol, that I've got to know through usage of *Keepalived*. VRRP allows transparent high availability (on L4) through introduction of virtual IP address, that can "flow" between multiple resources depending on their availability.<br> Let's assume a scenario, that we have web service, that we want to have highly available facing the world via 2 instances - one active, one redundant. By design those instances would have different adresses so we would need to know both of them. We can however make them available through VRRP and single virtual IP address and make implementation like Keepalived keep track if current "master" is available.<br> Implementations like Keepalived have all of the VRRP set up via configuration file (easy to automate!).<br> All nodes in such HA cluster are corellated through `virtual_router_id`, that they share. Node is promoted to master (primary) if it has highest `priority` from available nodes in cluster. If we have stalemate on priority, highest IP adress is picked as master.<br> When master is **elected** with VRRP protocol, it will announce itself through `ARP` (L3), that cluster's virtual IP address points now to its MAC address.<br> Of course you'd like to have some load-balancing solution instead of having single instance serving all of the traffic. Keepalived plays well with solutions like *HAProxy* to serve such purpose.
M|Container image security - tags vs digests|Engineering/Security|M|SBOM and supply chain analysis become a de facto standard for risk, vulnerability and licensing analysis for every company's stack through tools like *Dependency Track*. For such analysis, external dependencies are by default evaluated by their versions. <br>In containerized environments, images are versioned through the use of tags. Problem that it introduces is that tags in container image registry are actually mutable(!). If we push another version of an image with same tag, it will become a new image that version tag points to. Due to this, we cannot assume that reports like vulnerability scans are valid at the time of deployment to production, since the content of the image with same tag can differ. For orchestration scenarios like k8s, this can also lead to multiple instances of same pod running actually different applications, if images were referenced by tag.<br> To get rid of tag versioning problems we can use `image digests`. These are hashes of image manifest (calculated with SHA256), so its actual contents and dependencies. With such approach we can reference images by their content (think record types) instead of reference (think classic reference types). <br>Since digests are representing content of manifest, we can think of them as immutable - hash will only change if contents changed. When we reference them with digest hash, we are guaranteed to receive same content every time. Digest can be referenced in CLI or configuration files in a same way as tags, so instead of `docker pull ubuntu:20.04` we can use `docker pull ubuntu:sha256:7cc0576c7c0ec2384de5cbf245f41567e922aab1b075f3e8ad565f508032df17` etc.
M|Proxy configuration discovery (WPAD+PAC)|Engineering/Computer networks|M|In most enteprises proxy is a default middleware requesting resources from outside world on behalf of company users. One of a standard ways to determine whether user request should go through web proxy is to use Proxy-Auto Configuration (PAC) script which contains JavaScript function `indProxyForURL(url, host)`. Before automation for bootstrapping and updating configuration for machines became a standard, it was quite troublesome to have such up-to-date config everywhere. That's why Web Proxy Auto Discovery (WPAD) protocol has been introduced. Computers in given network will attempt to discover PAC file through WPAD protocol via DHCP and fallbacking to DNS (and to NetBIOS on Windows). Interesting part is that some transports like *WinHTTP* does this on Windows on OS level, and some BCL networking classes, like .NET's `HttpClientHandler` will attempt to discover proxy on it's own, regardless of OS on which it runs.
M|SANS25 (CWE/25)|Engineering/Security|H|SANS Institute is an information security organization offering tranining, education, certification etc. in cybersecurity space. Recently, its SANS25 list (made it together with Mitre organization, that is also responsible for CVE collection) made it to static analysis tools, to be used along with long-cherished OWASP TOP 10. Both of these lists revolve around programmers' mistakes, that lead to potential security exploits in software. While OWASP TOP 10 seems to be more broad (and generalized) with its points, SANS25 tends to be more granular. Because of this, SANS25 looks much more practical to me: there are related issues, there are code examples what is wrong, there are steps in software development lifecycle where such mistake can be mitigated and possible ways of detecting such issues in your pipeline.
L|Hyper-converged infrastructure (HCI)|Engineering/Infrastructure|L|Hyper-converged infrastructure is type of infrastructure, that is fully (or mostly) software-defined. It means that not only machines but also storage and network is software defined. Because of this most of the crucial infrastructure resources can be simply federated/shared across multiple workloads which helps to lower down total cost of maintaining on-prem infra. It also means, that maintenance of all of the infrastructure is simplier, since it can be managed on software level (so in more coherent way).
M|ConfigMap vs Secret in k8s|Engineering/Distributed systems|M|Both `ConfigMap` and `Secret` is a way of setting application (or even cross-application) configuration parts independently from their container images or pod configurations. Both ConfigMap and Secret are commonly utilized in deployed applications through either enviroment variables (`spec.containers[].env[].valueFrom.configMapKeyRef`) or via creating files representing Secret/ConfigMap keys in pod's volume (`spec.volumes[].configMap.name`) and then mounting them on container level (`spec.containers[].volumeMounts[]`). Note that only mounted ConfigMap/Secret will update pods automatically (for .NET it's however broken until .NET 6, since FileWatcher of config files do not follow LastUpdateDate of symlinked file, but only of symlink itself). Env variables will not update until pod restarts. ConfigMap and Secret are both kept in `etcd`.<br>  For ConfigMap and Secret, the main difference is - obviously - the fact, that `Secret` is meant to hold confidential data like username and password, API keys etc. Fun fact is that... they are mostly the same on implementation level - for now. Secret actually holds values as Base64-encoded, but that's not any protection at all. From my understanding, k8s creators meant this to be intent-based API semantics, that let users know, that secrets might eventually be encrypted somehow indepently from `etcd` (this seems to be an ambiguous case already, since k8s docs state that secrets are only encrypted, whereas docs of platforms like OpenShift state that they encrypt ConfigMaps too...). <br> One of the pitfalls on going "the right way" with splitting confidential and non-confidential configuration into Secret and ConfigMap is that they cannot be referenced from one to another. That might be an issue, if we want to have a single source of truth - like part of some node in config (provided it's JSON, XML, YAML etc.) that comes from ConfigMap (database address for example) and the other part comes from Secret (database login and password). We cannot do this right now (at least if application cannot merge such disjoint configs itself). Current solution is to simply throw everything together into Secret.
M|Richardson Maturity Model|Architecture/Distributed|M|So I've found out recently, that proper way of (or maturity in) designing RESTful APIs is based on an actual model, called **Richardson Maturity Model** that consists of 4 levels (from 0 to 3).<br> Classic RPC-like calls over HTTP to a singular endpoint are level 0 (doesn't resemble REST at all). Using address/routing to describe resources takes us to level 1. Going further to using HTTP verbs and HTTP status codes to describe access to or modification of a resource and actual response to such action makes it to level 2. That's what most developers got to learn and stop here.<br> Level 3 is where hypermedia "navigation" comes into play. HATEOAS (Hypertext As The Engine Of Application State) allows us to describe resources in REST not only via attributes but also via hyperlinks to both operations that can be performed and other resources, that are related. This could be described as ability to "discover what to do next" on resource-level. Sort of like describing resource as HTML document with links.
M|When to use external service discovery like Consul in k8s?|Architecture/Distributed systems|M|When going distributed, we nowadays think of containerization with Docker and orchestration with Kubernetes as an obvious choice. While those technologies mostly offer building blocks for building distributed platform, they can also carry supplementary services running on top of them. One of them is Hashicorp Consul, that can be used for: <br>- service discovery, <br>- service-to-service access control,<br>- integrating with config and secret store - Vault, <br>- K/V store. <br>While all of those are available in k8s tooling (in more or less mature way), Consul allows deployment on both Kubernetes and outside of it. This can be crucial for two approaches: <br> - preparing for containerizing your workloads and going to cloud when working on on-prem; integration with Consul and Vault can be later on moved to cloud without much of a changes regarding service discovery and configuration. This is especially true when you know how tricky to adopt k8s is on-prem and that you're not gonna adopt it anytime soon <br>- connecting applications from inside and outside of a Kubernetes cluster; for enterprises - regardless if in cloud or not - this is a common issue that needs to be solved. Consul offers that.
M|Distributed locking with Redlock|Engineering/Databases|M|Redlock is a distributed locking algorithm based on Redis, that is implemented in multiple programming languages. Redlock had three properties taken into consideration when created:<br>1. mutual exclusion, <br>2. deadlock prevention (via auto-expiration), <br>3. fault tolerance when majority of cluster is up. <br>Most of the naive distributed locking algorithms assume properties 1 and 2 but not 3, since they aquire the lock on single master node only. The problem is - if master crashes before properly replicating lock to slave nodes then we have a mutex violation because: <br>- lock holder still thinks lock is in place, <br>- since another node that did not get lock replicated got promoted to master, another processes think that resource is not locked and acquire lock themselves. <br> Redlock algorithm assumes locking on 3+ independent master nodes (so we are talking cross-cluster). Locking a resource is considered successful when lock is acquired on majority of participating masters, so `(N+1)/2`. <br> Martin Kleppmann (yes, the one that wrote *Designing Data-Intensive Applications*) argues, that Redlock is not actually safe, since there are issues like pauses (for ex. GC "stop the world") on client-side, that could lead to modification of some resource that lock was protecting when lock has already expired due to passage of time.<br> What Kleppmann proposes is to use some sort of global, auto-incrementing "fencing" token on storage side that would be checked on write. If I understand correctly, this sounds like optimistic concurrency control to me.<br> Redis and Redlock creator Salvatore Sanfilippo (Antirez) argues, that Kleppmann's arguments are not entirely correct. It seems that community consensus is when you asssume such long pauses may occur then consider setting long enough TTL on Redlock-based lock. It would not protect you 100% of a time but in most cases it will make possible for locking process to resume when lock is still in place. This of course has some flaws too, since processes that attempt locking will have to wait for auto-expiration of a lock when its holder crashed.
L|kubenet vs CNI|Engineering/Networking|M|When using k8s, in most cases we would not bother with networking aspects of infrastrcture - especially in cloud environment. <br>By default most (if not every) k8s managed platforms use `kubenet` networking, which offers most of the basic features that developers would expect from PaaS. On Azure kubenet based k8s node receive single IP address from Azure Virtual Network. Pods inside such node have non-routeable IP adresses and communicate with outside world using NAT. Such approach is desired for workloads that rely mostly on inter-cluster communication.<br> But in some cases we might need some advanced networking solution. That's where CNCF project called `CNI (Container Network Interface)` specification comes into place. On Microsoft cloud, Azure CNI allows allocating IP address from Azure Virtual Network address space which makes it possible for pod to be available from outside the Kubernetes cluster, for example by VMs in same subnet. Also lack of NAT removes additional hop in network and remove issues that sometimes NAT introduces. Since pods now are part of AVN subnet, Network Security Groups (NSG) can (and will) be applied.<br> Azure CNI of course comes at a cost - k8s consume lots of addresses from subnet, since it promotes horizontal scaling. One need to plan for such address space usage before going all-out with CNI.
H|Azure Pipelines - Variable groups|Standalone software/CICD|M|Pipeline (definition) is a core building block of CI/CD based on Azure DevOps platform. One of the concepts required to make it work without external tooling are *variable groups*, that allow us to replace parts of application configuration or pipeline definition itself during deployment, depending of the environment we are deploying to.<br> Variable groups can be referenced in pipeline definition in `stage.variables[]` as `- group: my-variable-group`. We can later use them on any of the steps we want or we can use replace parts of application config with https://marketplace.visualstudio.com/items?itemName=qetza.replacetokens. The way it works is sort of the same I've been doing it for years: it takes the config file, looks for template placeholders that match the start and the end tokens inside application and replaces it with value coming from variables in pipeline.<br> So for example: given `${` start and `}` end it will find single variable in this JSON: `{ 'MyDb': '${My_Variable_From_Variable_Group}' }` and will replace it with *My_Variable_From_Variable_Group* present in pipeline, imported from variable group. <br> What comes in handy in such approach is that we can define single template in our app configuration file once and the replace them for each of the environment, just by importing different variable group - distinct for given deployed environment.
H|k8s services behind the scenes: kube-proxy|Engineering/Networking|M|`Services` are one of the building blocks of Kubernetes stack. In most cases we would not care how they are implemented behind the scenes, but as always - some sort of mechanical sympathy plays part here. Services per se are mostly a "declaration of intent", sort of descriptor of what we want k8s to achieve for us without playing with tiny little details.<br> Behind the scenes these are enforced by so called `kube-proxy`. In the past, default mode for proxy was `userspace` mode, that made kube-proxy work as a "sort of reverse proxy" in front of pods. Nowadays, default (by choice at least) implementation is... `iptables`. <br> On every Kuberenetes node there is a `kube-proxy` deamon running that analyzes defined services (from control plane) and reflects traffic they describe in iptables rules. Interesting thing is that iptables is mostly a firewall and what's more, a service that reads all of its rules (that are additionally chained) in sequence(!). This can impact larger clusters where more modern solution can be used called `ipvs`.<br> So, how does `iptables` implementation actually looks like? To allow having single IP address registered in DNS for all of the pods matching *service selector*, we have a *Virtual IP Address* assigned to Service. This address targets multiple `Endpoints`, which represent actual IP addresses of pods behind the service. Then, for a source port that service exposes there is a iptables rule, that randomly (with `statistic mode random probability 0.xxx%`) load balances traffic to each of the pod, by their ip address from service's endpoints and target port from service descriptor.
M|Bloom filter|Engineering/Computer science|H|**Bloom filter** is a probabilistic data structure that aims to answer the question: *does element is present in a given set*? Bloom filter uses a bit array of size `m`, that represents previous occurrences of hashes of an input, outputted from `k` hash functions.<br><br> Let's take an example: we have string inputs, that get hashed by 3 hash functions (that I wrote about before): MurMur, FNV and SipHash and we have bit array of size 20.<br> Now, for each new string in our set we ran it through our hash functions (let's assume we got three outputs: 3, 10, 23). Output of hash function is used to set bit flag in our bit array (so we would mark `bit_array[3]`, `bit_array[10]` and `bit_array[13]` with value `1`). Since we can get hash greater than bit array size `m`, we choose position `hash_output mod (m-1)`.<br> Now, when we want to check, whether element is present in set, we run it through the same hash functions that we used to insert elements to set and check if bits in bit array are set to 1. If yes, then element is present in a set with **certain probability of false positives** (that can be calculated from size of `m` and `k`).<br><br> Because of its properties, Bloom filter is extremely space and time efficient structure, since regardless of input size it's bound to take `O(m) space` and `O(k)` time (times hash computation time ofc) to produce output. The cost is - of course - correctness.<br><br> There are several use cases for such interesting structure, most notably being:<br>- CDNs with a cache filter (*"should I cache this?"* aka `one-hit-wonders`),<br>- reducing disk lookups in databases (PostgreSQL and Cassandra are some of the notable examples)<br>- filtering out already recommended articles (Medium used this approach in ~2015),<br>- analyzing if URL should be additionally verified for being malicious (Google Chrome used to do this)<br> and so on...
M|Bicep for Azure|Engineering/Infrastructure|M|`Bicep` is a DSL for defining resources to be provisioned in Azure. Behind the scenes it compiles (or rather transpiles) into JSON-based ARM templates, so there's no "real magic" besides more concise syntax.<br> What is interesting, is that it's much more readable and moves towards "Terraform-way" of doing things with idempotent deployments and *what-if* analysis. While Hashicorp's tool uses file-based `.tfstate` state to track what to deploy and what not, Bicep calls Azure Resource Manager directly and diffs what should actually be done and what not. Microsoft also claims that Bicep being simpler and more declarative than ARM templates allow Resource Manager to parallelize some work (but dunno if that's true). <br> I need to play around with it a bit to see it's worth, but it's definitely a worthwile replacement for ARM templates. As for now, I prefer Terraform though since knowing it I can use my knowledge elsewhere too (on-prem, AWS, GCP etc.).
M|Partitions in Azure Service Bus|Engineering/Distributed systems|M|By default most of the queueing systems use single message broker backed by single message store. While this seems to be optimal for average throughput, it can lead to resource exhaustion even when we keep scaling up the broker. One of the proposed solutions in Azure stack is to use `partitioned queue` or `partitioned topic`.<br> The way it works is that it backs up actual queue or topic with **multiple brokers and stores** - one per partition (in Azure they're called `fragments`). Due to this its possible to scale them individually and - possibly - "keep going" even when one of them fails (but that's mostly solved by single-broker clustering solutions anyway).<br> Partition where message lands can be influenced by `MessageId` or `SessionId` if present (or by explicit PartitionKey). Otherwise random partition is chosen using round-robin behaviour (with PartitionKey generated internally by Azure Service Bus).<br> From the client side consuming messages from queue/topic, partitioning is transparent. Messages are collected from every partition and served to consumer (so something like *scatter and collect*). <br>Important thing to remember here is that `peek` operation on ASB queue will be the oldest message from partition, that first served message, not an actual oldest message on queue!
M|Azure managed identity|Architecture/Security|M|Azure `managed identity` is a way of introducing **credential-less** auth for VMs, Functions, AKS, App Services etc. to access Azure resources like Blob, SQL, KeyVault, other apps etc. Back in the days developer was forced to create `service identity` for application to access Azure resources and was forced to manage it credentials and lifecycle (delete when it's no longer needed etc.). Managed identity solves this problem by wrapping all of the identity management processes around service principal and making developer deal with only what can which identity do in Azure.<br> There are two types of managed identities:<br>- **system-defined identity** - tied to specific object, that is created with it and deleted with it (so it's an integral part of it); you would use this for single-instance, application-specific access<br>- **user-defined identity** - that is created independetly from an object and its lifecycle is not tied with object its assigned to; you'd use it for class of applications, that should have consistently applied access to given resource, regardless of how many of them are there<br> Usage of managed identity is pretty easy from code, since `DefaultAzureCredentials` from `Azure.Identity` uses automatically environment-based auth and (if not present) then managed identity associated with application.
M|Pets vs cattle analogy|Architecture/DevOps|M|Pets vs cattle analogy is a matter of distinguishing between servers, that need to be named explicitly like pets (think load balancers, SQL servers etc.) and named as a "yet another member" like in cattle (think application servers, NoSQL databases etc.). This analogy is important to minimizing infra resources that need to be manually tracked (pets) in favor of components that come and go (cattle) as we scale-out (and scale-down) and kill and recreate components in order with immutable infrastructure concept and supported with infra-as-a-code.
H|Podman as an alternative to Docker|Engineering/Operating systems|M|I've taken interest in alternatives to Docker that are still able to work with OCI-compliant images when wanting to run `docker` in WSL2-based Ubuntu. **Podman** seems to be the most popular alternative and it offers (as opposed to docker-cli) root-less support out of the box and is also architected as daemon-less (which ends up as a little pain too, since you need to start your containers on OS startup).<br> Podman support Container Image Format from OCI, so behaviour should be the same as docker-cli. <br>Podman also mirrors docker-cli API, so one can simply alias docker with `alias docker=podman` and use podman without learning new API. <br> Using podman in WSL (as opposed to Docker Desktop) does not require license for commercial use.
H|Container runtime building blocks|Engineering/Operating systems|H|Container ecosystem from developer perspective might be pretty obvious through tools like Docker (`docker-cli`) for development and Kubernetes for running in production. But how do these things run behind the scenes? First of all, back in old times Docker was a monolithic engine and that's what Kubernetes used.<br> With growing popularity of containers, things like container runtime (how to run containers) or specification of container image, its env and lifecycle were abstracted away. <br>They came in a form of `CRI` (**Container Runtime Interface**) which is a Kubernetes API for communicating with underlying runtime (that's what `kubelet` uses as a "contract"). Most popular implementations of CRI are `containerd` (which is also used by Docker) and `cri-o`. CRI implementations are often called "high-level runtimes", since they are delegating actual "container running" to OCI-based (**Open Container Initiative** Runtime Spec), "low-level/native runtimes" like model implementation of OCI standard: `runc`. <br> Last one thing worth noting is that "Docker containers" as we're used to call them are actually (for a few years already) OCI-compliant images, at least that's what all of the images must be compatible with.
M|Mermaid for Markdown|Standalone software/Documentation|M|For a long time I've been testing and evangelizing documentation that is part-of or at least close to the code it documents.  There are approaches like `PlantUML`, `arc42` or `docToolchain` that aim to bring docs-as-a-code to life.<br> Another new tool that brought my attention is **Mermaid** which is a tool based on Markdown syntax (which I really like and am using to write this particular note atm). It's syntax is pretty straightforward, which allows to easily "get devs on board". For example simple sequentional block diagram with three steps is literally ```graph TD; A-->B; B-->C```.<br> What makes it also great, is that it's currently supported by major Git providers, ie. GitHub, GitLab and Azure DevOps. So for most software houses with repositories adopting it is almost a "no-brainer".
M|StatefulSet in k8s|Engineering/Distributed systems|M|**StatefulSet** is a "variation" of another resource available in Kubernetes - ReplicaSet. It's meant for use with stateful applications (like databases or message brokers with persistence) that need to have unique, stable and "predictable" identity, non-volatile storage and that need proper and timed orchestration. <br> They also offer possibility of peer discovery va DNS of members of such StatefulSet, which is important for clustering to actually work for things like ElasticSearch, Redis or RabbitMQ, since cluster participants must be able to communicate with each other directly (not through round-robin balancing offered by classic Service resource). 
M|`CausationId` vs `CorrelationId`|Engineering/Distributed systems|M|When talking about a distributed - either synchronous or asynchronous - systems communication, it's hard to not talk about general observability and how parts of the process and reactions to its outcome end up connecting into a much bigger picture.<br> Solution to this is obviously distributed tracing. In general we would use a `CorrelationId` which allows us to connect several messages into one, complete conversation. For first message `CorrelationId = MessageId` and for every other message in conversation `CorellationId = previousMessage.CorrelationId`. <br> There is however one more identifier, that allows us to investigate what were the immediate/direct effects (aka what did X caused?) of the message. This id is `CausationId` and in conversation flow we would assign it as follows: `CausationId = previousMessage.MessageId`.
H|Vertical and horizontal partitioning|Engineering/Databases|M|So I've always thought that sharding and partitioning are seperate thing but.. it's seems they're not.<br> Sharding is actually one of partitioning types, notably `horizontal scaling` (HP) which we use to spread (shard) subset of of table/collection/etc. entries throughout many databases based on some key. One could visualize it like literally cutting horizontally table that would result in N parts with **same schema** but subset of source table data.<br> What we (often) think of when using the term partitioning is `vertical partitioning` (VP), which cuts vertically through table/collection/etc schema, resulting in N tables with subset of source table columns. Actually, **normalization** in RDBMS can be thought of as one of approaches to vertically partion data. Of course one can vertically partition already normalized table further if there's a need for that.
H|Lambda architecture|Architecture/Data|H|`Lambda architecture` is - by time of writing this - a de factor standard architecture for big data solutions. It's an approach to serve large amount of data for analytics and machine learning with fine balance between latency, throughout and fault-tolerance. Lambda architecture is meant to work on append only, immutable, timestamped data source. <br> It consists of three layers:<br>- `Batch layer` uses common batch-processing solutions (like **Hadoop**, **Redshift**, **Big Query** or **Snowflake**) to precompile  easily accessible views based on full dataset. If we think about its nature, we'll realize, that while being accurate it cannot serve the most up-to-date data fast (since the recomputation is needed);<br>- `Speed layer` works in parallel with batch layer to serve "live" data streaming (with solutions like **Apache Spark** or **Apache Storm**) to compensate the fact, that batch layer lags behind; speed layer deals very well with smaller, live dataset offering low latency for arriving, up-to-date data by compromising on throughput that batch layer offers;<br>- `Serving layer` makes it possible for speed and batch layer to work together by storing the results of them; depending on the data, many storage solutions can be used. For speed layer we'd use NoSQL solutions like **Apache Cassandra**, **Apache HBase** or more general use **MongoDB**, **Azure CosmosDB** or **Elasticsearch** depending on our needs. For batch layer most popular solutions seem to be **Apache HBase** or commercial **SAP HANA**.<br> While being the most popular big data architecture, Lambda architecture has been criticized for its complexity. There has been alternative architectures proposed, mainly `Kappa` architecture that work solely on data streaming solutions that offer backing storage (like **Apache Kafka**) and `Delta` architecture, that is a modified Lambda architecture that assume only incremental (hence "delta" name) data processing making the internal data mutable and allowing batch layer to offer much lower latency.
M|Log sampling|Architecture/General|M|`Log sampling` is a technique used to deal with large amount of log data by taking randomly (or more specifically) chosen entries from log and - by using statistical methods - presenting general overview oif the state of the system that logs come from.<br> Sampling makes it easier to manage logs - both from human but also cost (storage, I/O) perspective without compromising too much on actual observability of the system. That is, given that we sample enough data to have data that is somehow representative.
M|Logs vs metrics vs traces cheatsheet|Architecture/General|M|The three pillars of observability, known to most of the developers are:<br>- **logs** (either unstructured plain text or structured ones) that are records of events that happend in a given system (or in all of the systems if centrallized in ELK stack etc.),<br>- **metrics** that are simply numeric values read on given timestamp, representing usage, utilization or any other valuable characteristic of a system that can be used for monitoring and observability; most used ones are ofc CPU consumption in core percentage, memory consumption in bytes, request latency in ms and so on,<br>-  **traces** that are collections of spans (parts of a request pipeline) that allow us to identify hops and/or process forks, that are especially useful in distributed systems where what calls what and when (and how) is often non-trivial to deduct; <br> More tricky part is how to really use them and what should and should not they consist of?<br> For example logs should have no sensitive data, at least on production-data environments. Ideally they should be structured (either out of the box or at least by some ingestion engine like Logstash).<br> Metrics are a pretty basic tool but it's nice to know that they are divided by usage as counters (for ex. total requests served), gauges (CPU usage - high or low) and histograms (how metric A were changing throughout the time, for ex. how CPU usage looked like today on each hour).<br> Traces are the youngest ones to the party but are also the most complicated. Mostly due to the fact that fitting them into legacy parts of the system can be hard. Also it was non-trivial to connect many possible schemas for tracing to play together nicely, but this has changed 3-4 years ago with OpenTracing.<br> And finally - after archiving OpenTracing in CNCF - all of three have full-blown standard - OpenTelemetry - which of course needs yet another note to be properly covered.
M|Pareto frontier|Architecture/General (or management skills?)|L|Most of the engineers (but also economists, managers and scientists) know the `Pareto rule`, right? 20% targeted effort is responsible for 80% improvement, 80% of software can be done in 20% time (since its trivial and the rest will take 80% even if it consists of 20% total features) etc.<br> But recently I've heard about `Pareto frontier`, which is a most optimal solution given X objectives. When we say we've reached/approached Pareto frontier it means that when we'll improve one of the X objectives that we've set, the rest of the objectives will proportionally worsen.
M|Deployment strategies in k8s|Engineering/Infrastructure|M|A Kuberenetes basic but I got to recall myself few times already what does what, so... here it goes. There are two strategies for deployment rollout: `Recreate` and `RollingUpdate (default)`. <br> I believe both always create new ReplicaSet, so that a rollback is possible based only on state available in k8s.<br> `RollingUpdate` is a default strategy meant to gradually scale down current state (replica count) to acceptable minimum (so that total available pods do not go below `maxUnavailable`) while gradually scaling up new state (replica count of new replica set).<br> Of course for this to work properly applications must support two versions of application working simultaneously (otherwise they might for example do not work with new database schema etc.).<br> If it's not possible, we're left out with `Recreate` strategy which immediately scales down current state to zero and begins rollout of new state.
H|Kafka basics - cheatsheet|Engineering/Middlewares|M|For general, everyday throughput of business transactions that need some async channel, RabbitMQ, ActiveMQ or cloud-native flavours such as Azure ServiceBus or AWS SQS are good enough. However, when you're going for full-blown event-driven architecture that needs to support both classic OLTP but also BigData and other fast flowing streams, you're soon going to hit a wall with how they perform. That's when most of the IT teams switch to (or additionally augment their infra with) `Apache Kafka`, born at Linkedin and donated to Apache Foundation.<br> Kafka supports structures/mechanics already known from event-based communication like "brokers", "topics", "publish/subscribe" etc. However, where Kafka vastly differs from other message brokers is that it's so called **dumb broker** (resulting in **smart consumers**). It does not much job for developers themselves when compared to brokers like RabbitMQ, called often **smart broker dumb consumer** approach, which deal with things like delivery guarantees, dequeuing logic, retries, routing etc.<br> Kafka  can be thougth of (or at least I like to think about it) as a distributed store with broker logic put "over it". Main building block in Kafka is a topic. `Topic` in Kafka is simply an event stream at rest (serialized set of events). Topic is further organized into `partitions` that allow us to - from client-side perspective - organize and then consume events efficiently.<br> Partition is a main concept that allow scaling Kafka. Let's imagine IoT data coming over from vehicle fleet. Given an event stream partition by (partitionKey) - called, let's say - *vehicleId*, we can concurrently consume events from N paritions, where N = number of unique vehicles. It's possible because of a concept called consumer groups. <br> `Consumer groups` are a way of organizing consumers (so literally "some code" in consuming app) so that they can divide the job of consuming from topic between them without getting into each other's way. Let's go back to IoT example from before - we have N partitions, since we have N unique vehicles in our fleet. Let's assume N=5. Now given a single consumer groups of C=3 consumers, we will have them (automatically) assigned to partitions in our topic. So probably C1 will have partitions [P1,P2], C2 partitions [P3, P4] and C3 partition [P5]. Every consumer will exclusively receive messages from partition it was assigned to. Of course we need to wisely choose number of consumers, since if number of consumers is smaller than number of partitions, we will have idle consumers with no job to do. Also, topic can have more than one consumer group consuming from it.<br> Since we already know about topic, partition, consumer group and the fact that Kafka is - in a way - a dumb broker put over a distributed store, then how consumers deal with the fact, that they need to know which event do they processed last (so which one is next to process from stream)? The answer is: `offset`, which is a "pointer" on the topic's event stream itself. Consumers read from the partition and then mark where they've "left off". In early days of Kafka clients used to put this info into `Zookeeper` itself (a core Kafka dependency for cluster coordination) or into Kafka itself. From what I've found from current behaviour of clients, consumer offset is now being put into special Kafka topic called `__consumer_offsets`.
M|Azure CosmosDb - point read vs query|Engineering/Infrastructure|M|When we talk about cloud-native solution on platforms like Azure, we often forgot about the cost they might imply when used unproperly. CosmosDB, a multi-dialect database native to Azure is a great example of this, due to its distributed nature.<br> To achieve very low latency, CosmosDB (which is actually a document database at heart) takes advantage of partitioning, to efficiently spread data (and the load) across its physical nodes. That of course makes it tricky when we do not think on how we query data from such database. If we issue an operation to fetch data without stating partitionKey in our request, we will probably end up in costly, fan-out operation called (cross-partition) `query`. Depending on its complexity it will cost us a lot of RU - internal cost unit for CosmosDB.<br> To make sure we do not spend more money than necessary, we must do our best (since it might not be always possible), to issue requests that have partitionKey in them. They will result in `in-partition query` that might either have very low cost (if we use additional filters along) or in `point-read` operation (literally key-value read by key), that has exactly 1RU cost (if row size < 1KB) if simply requesting by partitionKey.<br> Additional how-to on modelling CosmosDB for read performance can be read here: https://docs.microsoft.com/en-us/azure/cosmos-db/sql/how-to-model-partition-example
H|BASE|Architecture/Data|L|`BASE` is an acronym for **Basically Available, Soft state, Eventual consistent**. It's an approach coined for NoSQL databases as a contradiction to the ACID acronym used to characterize relational databases. It basically says that it mostly does guarantee availability and changes over time without input, due to its eventual consistent nature. The term BASE was coined by the same person that formulated `CAP theorem` - *Eric Brewer*. It can be said, that given CAP theorem three pillars - Consistency, Availability, Partition handling - BASE gives up on consistency part.
H|What makes Kafka fast?|Engineering/Middlewares|H|Kafka is said to be one of the fastest middlewares/message brokers used by IT industry nowadays. But what makes it fast?<br> There is more than one reason why. It's connected to both the way it stores and reads data, but also how it divides responsibilities between client and server: <br> - as we know, Kafka is a distributed store or, to be more precise, *distributed append-only log*; because of it, both reads and writes are predictable and make it possible to take advantage of `Sequentional I/O`; even in SSD days this is significant, because most of OSes implement strategies like read-ahead (read more than requested) and write-behind (try to delay writes and group them together); random-access I/O can't take advantage of this (or at least not so much) <br> - Kafka uses approach called `Zero-copy` that avoids copying-over data read from storage to buffers on application level and then to network buffers by transfering data straight from storage buffer to network/socket buffer; to make it possible, Kafka (which is implemented in Java) uses `java.nio` features along with `Linux kernel 2.4+` features paired with network cards operation called `gather`<br> - Kafka's `consumer architecture` (*"dumb broker, smart consumer"*) allows the broker itself to drop most of the logic that other brokers (like RabbitMQ) implement; it also by default partitions consumers with consumer groups, that allows them to `not worry about other consumers concurrently reading the data` they're ingesting (and so they don't need to implement any logic to compensate out of order data which is expensive) <br> - Kafka `does not fsync (flush) received data before acknowleding the write`; this of course make it fragile in case of replica failure before flushing to disk. To mitigate this risk, Kafka makes sure that other in-sync replicas has the data in-memory too before acking the message; property that is responsible for this is called `acks` and can equal to *0* (do not wait for even leader to ack data), *1* (wait only for leader) and *all* (wait for leader and all of the in-sync replicas). Of course if producer decides that it does not want to wait for all of the in-sync replicas, then there is a possibility to lose the data in case of node failure.
L|What is superscalar processor?|Engineering/Infrastructure|M|`Superscalar processor` is a CPU, that given single input can issue more than one instruction per core. Superscalar processing is different from SIMD in a way that the former is `instruction-level parallelism`, whereas the latter is `data-level parallelism` (since SIMD operation on multiple data is still hardware-supported single operation). General idea is that there is more than one processing unit within a core that can accept input via instructions. `Hyper-threading` can be thought as an extension of superscalar architecture.<br> There is also an associated concept called `pipeling` that is complementary to superscalar architecture. Pipelining can be though as a assembly line in a factory producing a car:<br>- in a non-pipelined flow, we would have every step waiting for all the other steps of assemblying the car to complete<br>- in a pipelined flow we would assembly chassis and pass it on to wheel assembly section and start with next chassis, without waiting for rest of the car to be completed
H|Saga - orchestration vs choreography|Architecture/Distributed|H|`Saga` is a popular approach to deal with business transaction spanning across multiple services that - due to their distributed nature - cannot enlist themselves into one database transaction.<br> Saga (according to many people in industry) differs from `Process Manager` by the fact that it does not hold state, whereas Process Manager does. I think many people would use these term interchangeably but let's agree with this distinction so that in a future we can choose one of the other based on their implementation complexity.<br> Saga coordination (so essentially the pattern itself) can be implemented in two ways:<br>- using `Orchestration`, where multiple services participating in business transaction communicate only with single service/endpoint, that manages all of the routing resulting from steps in such transaction (so we have single point of implementation); general "plus" of this approach is that participating services do not need to (or even should not) know about themselves; also orchestrator knows of all of the parts of the process, so compensation on failure can be done in one place (knowing what went where and when); on the other hand, when orchestrator fails due some sort of technical issue, initiator of a business process has no way of knowing where the process failed and what needs to be rolled back, so this approach **actually needs state to know from where to rollback/compensate on orchestrator restart (and saga resume)** <br>- using `Choreography`, where multiple services, that take part in transaction talk with each without any sort of single point of coordination; this means that parts of a whole business transaction are scattered around the different services inside organization (or even outside of it); while this approach sounds much easier than Orchestration, it still has some challenges; let's assume process between three services: A->B->C->A; if the channel is synchronous, we might assume that on either failure or timeout of for example **"C"** in this chain, C must rollback itself properly, then B needs to rollback itself properly and then A needs to rollback itself properly; in general this should be doable, since in C if there is DB transaction, it would simply rollback on timeout if C's process is restarted; then on B we could catch C failure (via timeout or HTTP 4xx/5xx or SOAP fault etc.) and rollback B's transaction then propagate B's failure to A and so on... Choreography makes services in a business transaction aware of each other, even if they're decoupled on transport level (via message broker on async channels etc.), so that might be a "no" from architecture point-of-view if any unneccessary coupling is discouraged