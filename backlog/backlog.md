# Backlog

|Priority (L/M/H)|Topic          |Category       |Effort (L/M/H)|Collected notes|
------------- |-------------| ---- | --- | --- 
H|Merge 'Languages' and 'Frameworks' sections| - |M| - 
L|WCF connection patterns|Languages/C#|M|`Dispose` vs `Close`+`Abort`, proper use `ChannelFactory<T>`, caching (what and why)
L|SCIM2|Architecture/General|L|SCIM2 is an API open standard for cross domain identity management
L|PIM/PAM|Architecture/General|M|Class of solutions that manage elevated permission accounts and their sessions (ie. admins, super-users and so on)
M|CMM|Architecture/General|M|*Capability Maturity Model* - development model for assessing how mature particular processes are in certain organization/department/project; this can span from things like DevOps maturity, cloud-readiness, quality gates, code-review, self-organisation etc; measurement ("scoring") spans from 1 (Initial) to 5 (Effective)
H|SQL Server locking debugging|Languages/SQL|L|Expand current *T-SQL Performance Analysis Cheatsheet* with `sp_lock` and `sp_who` + SPID
M|XAdES|Engineering/Security|H|XML signing - expand on how it actually works "underneath"; used for **qualified electronic signature** (recommended by UE and MSWiA in Poland)
H|SQL Server - `UPDLOCK` vs `SERIALIZABLE`|Languages/SQL|M|`UPDLOCK` hint can be used instead of `SERIALIZABLE` isolation level to prevent duplicates `INSERT` in (`SELECT` then `INSERT` scenario); it can gracefully force other sessions (that also use this hint) to wait for their "chance" to SELECT without ending up with deadlocks (as in `SERIALIZABLE` case)
M|SQL Server - lock compatibility matrix|Languages/SQL|M|Describe compatibility matrix between Shared (S), Exclusive (X), Update (U) and all the Intent types (I...)
M|SQL Server - `RoundTimeDown`|Languages/SQL|L|Describe creating time buckets in SQL Server with `RoundTimeDown`; quite handing for reporting and all time series oriented data in SQL Server
M|OpenTelemetry|Architecture/General|H|Describe OpenTelemetry - it's goals, current state and usage; from their page it `is a collection of tools, APIs, and SDKs. You can use it to instrument, generate, collect, and export telemetry data (metrics, logs, and traces) for analysis in order to understand your software's performance and behavior.`
M|Microfrontends|Architecture/Distributed|H|Describe Microfrontends as a general approach to decomposing monolithic frontends and as a final step in *vertical slices* approach to succeed
L|Webpack Module Federation vs single-spa|Languages/JavaScript|H|Describe differences between these two approaches and how they can be used to achieve microfrontends when needed
M|TailwindCSS|Languages/CSS|H|TailwindCSS has been gathering large popularity for a long time already; investigate and write down in your words how it works and "why the hype"
L|StencilJS|Languages/JS|H|StencilJS has been recommended in many places as a go-to solution for Web Components (and building design systems with them); investigate and write down how in general it works
L|RBAC,ABAC,ACL|Architecture/General|M|Briefly desribe RBAC, ABAC (RBAC + attributes) and RBAC (business role - create, write, enslist, pay etc.) vs ACL (technical - has or has not access to resource)
M|Marshalling|Engineering/Distributed computing|M|Briefly desribe definition of marshalling and find few examples of it like RPC/REST (with JSON/XML/MessagePack etc.), COM etc; compare with serialization
H|Linux commands cheatsheet|Languages/Linux commands|M|Describe commonly used Linux commands, especially `find`, `grep`, `sed`, `awk`, `cat`, `touch`, `mkdir`, `ls`, `virtualenv` and others
M|x86 assembly basics|Languages/Assembly|H|"Serialize" notes from reading ***C.O.D.E.***; note down popular x86 opcodes to be not so ignorant when reading x86 ASM files
H|How computer works? - high level|Engineering/Computer Science|H|Create simple cheatsheet (with diagrams) on how computer architecture works, ie. CPU (ALU, registers and L1, L2, L3 cache), memory, disc, bus speed and ideas/problems around it, ie. cache misses, branch prediction, register spilling (moving to RAM) etc.
L|Shift left|Architecture/General|L|"Shift left" is an organisation approach to security, that encourages security review as early in development process as possible (so "left" means literally left on time axis given software development lifetime)
L|Sonar vs Fortify|Standalone software/CICD|H|Both tools are used for static code analysis; Sonar is used for for code quality analysis (and measuring technological debt), whereas Fortify is used for code security analysis; compare how they can be used together in CI/CD pipeline
H|Certificates in PKCS|Engineering/Security|H|Describe what role certificates play in Public Key Cryptography Standards (PKCS) infrastructure; describe what thumbrint (hashed certificate), signature, CA (Certificate Authority) and trust chain is; throwing in few words about asymmetric cryptography wouldn't hurt
M|Array pooling with `ArrayPool<T>`|Languages/C#|M|*Array pooling* is a concept of initializing large array beforehand (if we anticipate that we will be allocating large arrays and do it a lot) and then renting chunks from it when we need them; if we keep renting from once preallocated pool, we do not end up with lots large arrays on LOH (Large Object Heap), that can later on cause forced managed heap collection (which hurts performance a lot); `ArrayPool<T>` offers pooling and renting (with `ArrayPool<T>.Rent`) functionality in .NET and is thread-safe; Adam Sitnik has amazing post about it on https://adamsitnik.com/Array-Pool/
H|Small Object Heap (G0, G1, G2) and Large Object Heap|Languages/C#|H|Write down general mechanism behind .NET Garbage Collection, SOH (and its generations) and LOH; include topics like LOH compacting, memory fragmentation problem, when object lands on LOH and general LOH problems; definitely approach with Konrad Kokosa's book accompanying you
L|tcpdump on Linux|Engineering/Computer networks|M|Describe usage of tcpdump - network sniffer for Linux; could be worth comparing it briefly with Wireshark
L|TLS termination|Engineering/Computer networks|M|Describe TLS/SSL termination - how it works, why we do this (performance, network appliance, package analysis etc.)
L|FQDN vs hostname|Engineering/Computer networks|L|Hostname could be FQDN if it goes up to the top-level domain
M|Load balancing - L4 vs L7|Engineering/Computer networks|H|Describe how load balancing is implemented nowadays, especially comparing layer 4 and layer 7 balancing; if it ends up as not a technical but more general note, move to Architecture/Distributed
M|HAProxy|Standalone software|M|Play around with HAProxy and note some general concepts behind how it works
H|async-await vs locking|Languages/C#|M|Write down approaches to using async-await with locking (mainly `SemaphoreSlim`, but could also hackaround your way with simple bit flag and `Interlocked.CompareExchange`)
M|Raft algorithm|Engineering/Distributed computing|H|Describe Raft algorithm and how it achieves cluster consensus with distributed log, leader election etc; note actual uses in solutions like Neo4j (+ONgDB) or RabbitMQ (with quorum queues)
L|Byzantine generals problem (Byzantine fault)|Engineering/Computer science|M|Describe Byzantine fault problem and how it affects distributed computing systems
H|Event schema changes - challenges|Architecture/Distributed|M|Describe solutions to adapt to changing event schemas, especially with event sourcing approach, where multiple versions changing over time can't be guaranteed to properly deserialize to strongly-typed event type; solutions I can think of would be *upcasting* along with usage of *weak schema* (for ex. JSON); consider describing *snapshot* creation too
L|OSINT|Architecture/General|M|Describe (from high level perspective) what open-source intelligence approach is and why you should care in terms of information security
M|Business validation - close to input or in core domain|Architecture/General|M|Describe current thoughts on (especially basic, ie. form) validations outside of core business domain with libraries like `FluentValidation` or `MVC Model validation`, that can offload trivial checks like required, min/max length, pattern match etc; discuss challenges of (possibly) missed rule enforcement in domain model, that can lead to breaking invariants and persisting domain model in invalid state 
H|Angular - change tracking strategies|Languages/TypeScript/Angular|H|Describe how change tracking strategies work in Angular: *default* (with "deeper" checking and mutability support), *OnPush* (shallow, preferred for immutable data) and *disabled* (and manualy triggering using `ChangeDetectionRef`); describe briefly how `Zone.js` works with Angular; decribe how change detection mechanism work with templates (`foreach` + every *template binding* + compare previous value of field with current value but **only for those that are used in bindings**)
M|`[ThreadStatic]` vs `ThreadLocal<T>`|Languages/C#|M|Describe diffferences between `[ThreadStatic]` and `ThreadLocal<T>`; both are used for thread exclusive data (so each thread has its own copy), but `[ThreadStatic]` initializes for first thread only whereas `ThreadLocal<T>` initializes for every thread; `ThreadLocal<T>` also implements `IDisposable` (good for some cleanup); for keeping data local only to particular async flow (async-await chaing) you can also use `AsyncLocal<T>`
M|C# 9 records|Languages/C#|M|Describe how `record` - new C# (9.0) language feature - works; in general records have been introduced for implementing (reference) types, that are described by their data (so for ex. DTOs fit in here), offering out-of-the-box value equality. While meant mostly for immutable scenarios (by default they are immutable with auto-properties with `init` only setters), they can also be mutable. Records support nondestructive mutation (more like pure functional) with `with` keyword, that copy source record and apply given mutation on it, ie. `var secondRecord = firstRecord with { MyProp = "new value" }`. Another great productivity shorcut that records provide is a *positional syntax*, which allows us to define data-type in single line, ie: `public record Person(string firstName, string lastName, DateTime birthDate);` would yield encapsulated, immutable reference type with value equality implemented behind the scenes.
S|Angular - compare *dirty*, *touched* and *updateValueAndValidity*|Languages/TypeScript/Angular|M|Compare *dirty*, *touched* and *updateValueAndValidity* in Angular Forms and describe when and why should each of them be used.
H|Describe formal difference between *system* and *business* analysis|Architecture/Business analysis|M|Describe differences between system and business analysis. It seems that definition of business/system *analytic* and business/system *analysis* differ, as system analysis (according to Wikipedia and some other pages) is a subset of business analysis, which deals with translating organisation's needs, models and workflows into IT systems functionalities. On the other hand, it seems that business analytic does not deal directly with system analysis, even though it's a part of business analysis as a whole. Perhaps for practical reasons system analytic's work has been extracted from business analytic's responsibility list to allow division of work (and also specialization).
M|*Service blueprint* technique|Architecture/Business analysis|M|Describe *service blueprint* technique on how to plot a diagram of how particular service (let's take order as an example: order->pay->package->delivery) is being realized from high level perspective. Service blueprint consits of actors (placed on vertical/Y axis) and steps to deliver service (placed on horizontal/X axis). It also includes visibility boundry, which allows to distinguish processes that are visible to user (above boundry) and those, that happen "behind the scenes" (below boundry).
H|*Replication events* streaming via audit log or change tracking/change data capture|Architecture/Distributed|H|Describe my approach to building local storages around autonomous systems/modules/microservices etc. with audit log scanning or - if available - scanning Change Tracking (SQL Server) or Change Data Capture (SQL Server or PostgreSQL) and publishing results as either strongly typed (if in homogenous environment) or a weakly typed (if in polyglot environment) replication events.
L|`AsyncLock` by Stephen Cleary based on Stephen Toub MSDN article|Languages/C#|M|Describe a `AsyncLock` implementation, that simplifies mutual exclusion in async-await scenario (using `SemaphoreSlim` and `IDisposable` with `using` pattern behind the scenes)
H|Mutex, lock, semaphore, monitor, barrier, spinlock, RCU locks|Engineering/Computer science|H|Describe synchronization implementations used nowadays, including mutex, lock, semaphore, monitor, memory barrier/fence, spinlock, read-copy-update and readers-writer locks. It wouldn't hurt to mention read-modify-write, fetch-and-add and test-and-set operations. Why `volatile` keyword is important should also be mentioned here.
H|Compare HTTP Codes vs 200 + `{ errors: [] }` approach to business error handling|Architecture/Distributed|M|Discuss on how and when HTTP code for each class of problem (4xx) in RESTful APIs would be a better choice than 200s + errors in payload. Perhaps there should be some middle ground (especially for business validations) introduced as *catch-all* mechanism? I can think of returning every business error as 422 (Unprocessable Entity) to indicate business error (monitoring or client-side libraries auto-detect 4xx and 5xx as errors) along with payload decribing what went wrong. I'm typically against using HTTP codes in general as results of business rules violation, because I treat them as *technical errors*. So either input was malformed (serialization issue?), server refused to authorise user or simply such endpoint does not exist. Or maybe server just exploded and all we know is that 500 - Internal Server Error. However, I can understand how this can be misleading, that API returns 200 - OK for errors, even if there is a reason in a payload. When exposed to the outside world, I would probably go with 422 route + `{ errors: []}` approach.
M|SQL Server security model|Engineering/Databases|L|Describe basics of SQL Server security model: login, mapping login to user on particular database, creating role on such database, assigning user to it and assigning permissions to certain objects to that role.
M|What is Istio?|Engineering/Distributed|M|Describe what is Istio and what problems it solves, ie. traffic management (routing, policies, load balancing, service discovery, staging and rolling releases etc.), observability (metrics, healthchecks, distributed tracing, access logs) and security (mostly certs and authorization, for ex. JWT access rules on service-level); also it would be useful to compare Envoy sidecar proxy Istio utilizes on service-level with kube-proxy in Kubernetes that works on node-level; briefly descrive data plane and control pane in Istio.
L|Kubernetes Ingress vs alternatives (Traefik, HAProxy, Istio Ingress)|Engineering/Distributed|H|Compare popular Kubernetes Ingress alternatives and write few bullet points when it's worth switching to one of them.
M|Domain (private) events vs integration (public) events|Architecture/General|L|Compare domain and integration events along with use cases;domain events - internal communication in-memory between aggregates in same bounded context/service boundry; integration events - out-of-process for external services, modules or even systems that are meant to do something with it in async manner
M|Working set vs private set|Engineering/Operating systems|M|Describe differences between private set (aka private bytes), ie. memory that process allocated (and is in use or paged out) and working set, ie. memory, that process is actively using (in main memory). There are also virtual bytes, which stand for total virtual adress space used by process. This metrics can be helpful in troubleshooting OutOfMemory exceptions caused by memory leaks (private bytes > working set), memory fragmentation (virtual bytes > private bytes) or simply when allocating more memory, than can fit in RAM + pagefile.
L|SQL Server - Instance vs VM stacking|Engineering/Databases|M|Describe differences (in cost, licensing, administrating and troubleshooting) for provisioning SQL Server databases with instance stacking (multiple instances on one VM) approach vs VM stacking (single instance per VM) approach. From my current notes it's worth analysing SQL Server and Windows Server licensing (per core vs per installation), resource sharing, maintainability (when multiple instances can have impact on each other on same VM) and so on.
H|SQL Server - full, differential and transaction log backups|Engineering/Databases|H|Describe different types of backups, that SQL Server offers: full (`.bak`), differential (`.bak`) and transaction log (`.trn`). First of all, it would be essential to compare recovery models: `SIMPLE` (full and diff only) and `FULL` (full, diff and transcation log). Then, it would be worth describing why SQL Server has two (actually three but hey...) files behind the scenes: `.mdf` (Master Database File), that stores data and `.ldf` (Log Database File), that stores transactions, ie. changes to database. When change happens in database, it gets changed in-memory (in a buffer) and modified data is marked as dirty (dirty pages) and change is logged to transaction log in .ldf file on disk. Later on this change will be finally persisted to .mdf file asynchronously on so called `CHECKPOINT`. The reason why three types of backups exist is to prevent data loss and to decrease recovery time when disaster occurs. Full backup backups literally everything, that is currently in database. Differential backup consists of everything that changed since last full backup, so if full backup was on 1st day of month and you created differential each day, on 10th day of month you will have 10 backups, where 10th backup has all the 10 days (1d-10d) since full backup, 9th has 9 days (1d-9d) of changes sinces last backup and so on. Since differential backups (not to mention full backups) take time, transaction log backups are performed much more frequently (ie. per hour, per 15min - dependes od disaster recovery model in organization). The reason why we won't simply do full backup each X days + all of the transaction log backups is recovery time. In such model we would have to restore each transaction log in order, sequentially one after another after full backup restore. If we take transaction log backup per 15min, we would have to apply 96 .trn files after 1 .bak file to recover database from a backup day ago, instead of at most 16 .trn files, if we have differential backup every 4 hours. Why don't we give up on transaction log backups though (that's what SIMPLE recovery model actually is)? First of all, we can't practically make frequent diferrential backups on larger/busy databases effectively. Also, if we recall how data is actually written from .ldf to .mdf file, we can think of a scenario, when we lose data, that was not yet written to .mdf file (but that could be somehow mitigated with `recovery interval` settings on server to issue frequent checkpoints at cost of busy I/O).
M|Non-cryptographic hash functions (Murmur, FNV, SipHash)|Engineering/Computer science|M|Describe usage of non-cryptographic hash functions. Common functions are `Murmur`, `FNV` (Fowler-Noll-Vo) and `SipHash`; there is also `xxHash`, which claims to ran at close to RAM speed(!). Historically, most popular non-crypto hash function was `CRC32` (which is probably still in use, for example for data integrity check of zipped files). Since these hash functions are not (generally) tweaked to be used in security scenarios, they tend to be faster than cryptographic functions. They're usage nowadays is for data integrity checks (CRC and checksums), verifying uniqueness and for hash tables implementation (Murmur3 is used in memcached, Redis and ElasticSearch). Interesting comparison can be found o StackExchange: https://softwareengineering.stackexchange.com/questions/49550/which-hashing-algorithm-is-best-for-uniqueness-and-speed/145633#145633
M|Cache stampede|Engineering/Distributed|L|Cache stampede is a situation, when under heavy load multiple processes attempt to compute resource (for ex. web page) to cache upon cache miss. This can possibly lead to situation, when resource, that we attempted to cache will never be served to user due to timeout. This can also lead to general system instability to due to resource (CPU, network or IO) starvation. Possible mitigation can be locking or delegating resource-to-be-cached computation outside of the process (assuming it will be single place, it will know of all incoming requests to cache such resource).
M|Law of Demeter|Engineering/Design patterns|L|Law of Demeter (*"don't talk to strangers"*) is a rule, that states, that class C should talk (method call) only to it's own members, objects created by it, arguments passed to its methods and to global (static) objects. Therefore it forbids calls like `person.getAddress().getZipCode()`. Ofc in general getters are not an issue - actual business operations performed on objects other than "neighbours" are the issue here. They lead to coupling between objects, that is not so obvious when analysing given class.
H|.NET performance troubleshooting|Languages/C#|M|Describe commonly used tools to troubleshoot .NET applications performance. The ones that I've mostly used are *PerfView*, *DebugDiagTool*, *perfmon* with choses performance counters (depends on your usecase), *Visual Studio profiling session* (amount of calls and CPU time, managed heap structure etc.). There are also new tools in .NET CLI, especially useful on Linux environments, from which most of can be found on dotnet organisation on GitHub in diagnostics project (mainly `dotnet-counters`, `dotnet-dump` and `dotnet-gcdump`).
M|SQL Server - `WITH(NOLOCK)` vs `WITH(READPAST)`|Languages/SQL|L|Compare `NOLOCK` (== `READ UNCOMMITTED`) with `READPAST` hints in SQL Server. Both of these hints are used for speed optimisations, but they differ in a fact, that `NOLOCK` reads state of records, that are currently inserted/updated/deleted in a pending transactions (so *dirty read* occurs) wheras `READPAST` in a same situation will skip locked rows.
M|SQL Server - RCSI (Read Committed Snapshot Isolation)|Engineering/Databases|M|Describe how `RCSI` mode works in SQL Server. RCSI is used to achieve optimistic concurrency model in SQL Server by using version store in **tempdb**. All the current transactions running under (generally default) `READ COMMITTED` will now skip locking on read enitrely and will receive last committed version of read record from the moment of when transaction started. This does not changes how other isolation levels work (which is sometimes forgotten, by myself for example...). RCSI in most cases will increace concurrency of the database but will come at a cost of higher tempdb utilization (which should be closely monitored and perhaps some upgrades of this part of SQL Server setup should be planned). RCSI is often summarized as **writers do not block readers (and vice versa)**.
M|Indexed view (materialized view in SQL Server)|Engineering/Databases|M|**Indexed view** is a SQL Server implementation of materialized view. To create indexed view you need to create simple `VIEW` with `WITH SCHEMABINDING` option and `UNIQUE CLUSTERED INDEX`. Indexed view (as any other materialized view implementations) stores data physically, just like tables do (in contrary to classic views). When querying against remember to use `WITH(NOEXPAND)` hint on SQL Server versions lower than Enterprise. It will force query optimizer to use actually use indexed view instead of using table behind it. When moving from backlog, compare indexed view with Oracle SQL and PostgreSQL implementations of `MATERIALIZED VIEW`.
L|`WITH SCHEMABINDING` options|Engineering/Databases|M|Why use `WITH SCHEMABINDING` option in SQL Server? I've found two reasons so far: first of all, if we want to avoid accidental breaking of views when source table is changed, schema binding helps, because it forces DBA that alters the table to carefully review changes schema-bound view too (because view will have to be recreated). Second considerations is for UDFs that compute scalar values - when we use schema binding on them, we let know query optimizer to analyze body of the function and to possibly skip *Halloween protection* measures (like Table Spool) when it's not really needed (boosting performance as a result).
M|`CancellationToken` in ASP.NET Core|Languages/C#|L|Automatic binding of `CancellationToken` as a parameter to controller action in ASP.NET Core allows some interesting techniques in typical OLTP systems. When a request is aborted (for ex. user aborts request before it finishes), cancellation is requested on CancellationToken bounded to called controller action. While this might be not so useful on Controller level, we could pass it further, for ex. to Query/CommandHandler `Handle()` parameter and introduce some sort of transaction rollback (or skipping `SaveChanges` on `DbContext` or skipping `Complete` on `TransactionScope`), when we decide to commit, but find out that user actually aborted request. This might seem more natural to some of the processes, where user expected action to be aborted but modifications were still applied to the system.
M|`async/await` behind the scenes|Languages/C#|M|Describe how `async/await` works behind the scenes, what is cost of it (generating compile-time state machine and running it with queuing and scheduling continuation execution). It would be worth mentioning how state machine behind the scenes looks like, what optimisations are made for already completed tasks and why state machine is `class` in Debug but `struct` in Release. 
H|Optimistic concurrency control|Engineering/Databases|M|**Optimistic concurrency control** (OCC) is a way of guaranteeing correct results  when concurrently modifying state in database/filesystem etc. without locking the resource like database record (which would be a **pessimistic concurrency control**). It has its advantages (no locking, so no deadlocks and no need to manage them) but also introduce some challenges (managing versions in storage, passing them to user and back, introducing version mismatch strategies, ie. rollback or retry with user consent etc.). In typical HTTP web application, version of the record is sent to user using `ETag` HTTP header. Many ORMs include OCC out-of-the-box. For example `EF Core` supports it with timestamp-based **ConcurrencyToken** (either with data annotation `[Timestamp]` or with `IsRowVersion()` in fluent API). General strategy is to compare concurrency token on modifying state (`UPDATE` for example) in `WHERE` clause, ie. `UPDATE my_table SET xyz = 123 WHERE id = @id AND concurrency_token = @token`. If `affected records != expected records` (for single update: 0 != 1) concurrency control error should be raised. Optimistic concurrency can not only be used in classic `user -> web app -> db` scenario, but also help in deadling with distributed state in service-service (especially async) communication, when mismatched versions on requested operation should lead to message rejection or compensation (or both).
H|GRASP (General Responsibility Assignment Software Patterns)|Engineering/Design patterns|M|**GRASP** is a set of nine fundamental principles in object design and responsibility assignment, first applied by Craig Larman in *Applying UML and Patterns*. <br> Those principles are (P - problem, S - solution):<br> `Information Expert` (P: Where do I assign responsibilities? S: To objects, that has data to fulfill them),<br> `Creator` (P: Who creates objects A? S: Object of type B, if B aggregates A, B records instances of A, B closely uses A or B has required information to instantiate A),<br> `Controller` (P: Who should be responsible for handling input? S: Controller dedicated to given use case or group of related uses cases),<br> `Indirection` (P: Where and how to assign responsibility, to avoid direct coupling, while keeping high reusability? S: Introduce indirection through additional object responsible for mediation),<br> `Low coupling` (P: How to decrease impact when changing one class to other classes? S: Decrease direct dependencies between objects.), <br> `High cohesion` (P: How to keep objects focused, maintainable and understandable? S: Object should keep only those responsibilities, that are highly related to each other.),<br> `Polymorphism` (P: How to make components pluggable or replacable with alternative behaviour? S: Identify points of possible instability ie. alternative behaviour and introduce interfaces, instead of concrete implementations there),<br> `Pure fabrication` (P: How do we achieve rules like indirection without polluting problem domain? S: We can introduce intermediate classes, that do not represent problem domain per se, for ex. domain services in Domain Driven Design).<br>
M|Cohesion (programming)|Engineering/Design patterns|M|Cohesion in programming is a measure of determining *to which degree elements inside a module belong together*. When we mean that class has high cohesion, it means that its methods have a lot in common and do not process lot of unrelated data. Types of cohesion are as follow (from lowest/worst cohesion to best/highest cohesion):<br> `Coincidental` - parts arbitrarily groupped together in a same source file but not really common (*Utility* classes etc.)<br> `Logical` - parts groupped by type of logic they represent, for ex. all the *Controllers* in MVC pattern in same folder (logic - Controller, but different use cases/functionalities)<br> `Temporal` - parts groupped by moment in time in execution of program, when they are processed (ie. functionalities run on startup groupped together fit in here)<br> `Procedural` - parts groupped by sequence of execution (before X do Y)<br> `Communicational/information` - parts groupped because they operate on same data<br> `Sequential` - parts groupped, because output from operation X is input to operation Y (so opposed to procedural cohesion, this one is more of a neccessity); an example would be - I guess - companion methods to template method pattern?<br> `Functional` - parts groupped because they all contribute to the same, single task
M|Bus factor|Soft skills/Project management|L|Bus factor is a risk management measurement, that helps to determine single points of failure when key technical expert is gone and information/capabilities that person had, have not been shared (that's what name comes from: *what if person X get hit by bus?*). Bus factor is calculated by person count that would need to disappear from process/project/etc. to for it to stall. For ex. if in IT project single person has knowledge how compoent X works and such knowledge has not been shared (and is currently hard to catch up), we can say that bus factor = 1. If we'd have three person with such competences, bus factor would equal 3.